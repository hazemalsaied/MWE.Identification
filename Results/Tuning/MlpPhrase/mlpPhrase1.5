INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_PNZQO6.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 10869/11441 Mb (0.950000) on cuda
Mapped name None to device cuda: Tesla K40m (0000:03:00.0)
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp5n50Da and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpRnohmX and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp1dONqv and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpXjhN7e and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp3T4hMl and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpfgoPQc). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpWwrLRm and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmp6CCV3W and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpjkzbGA and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpqDJ1vM). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
INFO (theano.gof.compilelock): Waiting for existing lock by process '7176' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '13627' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5260' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5018' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5018' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5018' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5112' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5112' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5260' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5269' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '5802' (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,115            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.026          ,50             ,23             ,26             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
13             ,178            ,True           ,True           ,28             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 115, True, 0.026, 50, 23, 26, 13, 178, True, True, 28
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (14h:56)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 26)       196170      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 23)       3496        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 178)       1343010     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 13)        1976        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 50, 49)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 890)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 65)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 28)           6552        concatenate_1[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 955)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 983)          0           phraseRnn[0][0]                  
                                                                 concatenate_2[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 115)          113160      concatenate_3[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 4)            464         dense_1[0][0]                    
==================================================================================================
Total params: 1,664,828
Trainable params: 1,664,828
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 279s - loss: 0.0649 - acc: 0.9831 - val_loss: 0.0553 - val_acc: 0.9858
Epoch 2/40
 - 286s - loss: 0.0499 - acc: 0.9877 - val_loss: 0.0548 - val_acc: 0.9861
Epoch 3/40
 - 260s - loss: 0.0470 - acc: 0.9886 - val_loss: 0.0574 - val_acc: 0.9861
Epoch 4/40
 - 260s - loss: 0.0454 - acc: 0.9891 - val_loss: 0.0603 - val_acc: 0.9860
Epoch 5/40
 - 260s - loss: 0.0446 - acc: 0.9893 - val_loss: 0.0637 - val_acc: 0.9859
Epoch 00005: early stopping
	TRAINING TIME: 27.43 minutes 
==================================================================================================
	PARSING TIME: 7.92 minutes 
==================================================================================================
	Identification : 0.581
	P, R  : 0.735, 0.481

==================================================================================================
	XP Ends: 25/6 (15 h:31)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 25/6 (15h:31)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 26)       188266      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 23)       3887        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 178)       1288898     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 13)        2197        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 50, 49)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 890)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 65)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 28)           6552        concatenate_4[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 955)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 983)          0           phraseRnn[0][0]                  
                                                                 concatenate_5[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 115)          113160      concatenate_6[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 4)            464         dense_3[0][0]                    
==================================================================================================
Total params: 1,603,424
Trainable params: 1,603,424
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 180s - loss: 0.0721 - acc: 0.9803 - val_loss: 0.0599 - val_acc: 0.9842
Epoch 2/40
 - 179s - loss: 0.0523 - acc: 0.9868 - val_loss: 0.0593 - val_acc: 0.9848
Epoch 3/40
 - 179s - loss: 0.0488 - acc: 0.9880 - val_loss: 0.0628 - val_acc: 0.9846
Epoch 4/40
 - 178s - loss: 0.0472 - acc: 0.9885 - val_loss: 0.0666 - val_acc: 0.9838
Epoch 5/40
 - 177s - loss: 0.0464 - acc: 0.9886 - val_loss: 0.0683 - val_acc: 0.9848
Epoch 00005: early stopping
	TRAINING TIME: 15.8 minutes 
==================================================================================================
	PARSING TIME: 12.35 minutes 
==================================================================================================
	Identification : 0.517
	P, R  : 0.443, 0.62

==================================================================================================
	XP Ends: 25/6 (16 h:0)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (16h:0)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 26)       358618      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 23)       2507        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 178)       2455154     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 13)        1417        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 50, 49)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 890)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 65)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 28)           6552        concatenate_7[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 955)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 983)          0           phraseRnn[0][0]                  
                                                                 concatenate_8[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 115)          113160      concatenate_9[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 4)            464         dense_5[0][0]                    
==================================================================================================
Total params: 2,937,872
Trainable params: 2,937,872
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 356s - loss: 0.0654 - acc: 0.9824 - val_loss: 0.0563 - val_acc: 0.9846
Epoch 2/40
 - 353s - loss: 0.0513 - acc: 0.9869 - val_loss: 0.0570 - val_acc: 0.9846
Epoch 3/40
 - 354s - loss: 0.0483 - acc: 0.9880 - val_loss: 0.0616 - val_acc: 0.9843
Epoch 4/40
 - 354s - loss: 0.0466 - acc: 0.9886 - val_loss: 0.0663 - val_acc: 0.9844
Epoch 5/40
 - 356s - loss: 0.0453 - acc: 0.9889 - val_loss: 0.0739 - val_acc: 0.9840
Epoch 00005: early stopping
	TRAINING TIME: 32.28 minutes 
==================================================================================================
	PARSING TIME: 5.28 minutes 
==================================================================================================
	Identification : 0.476
	P, R  : 0.494, 0.459

==================================================================================================
	XP Ends: 25/6 (16 h:38)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,147            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.046          ,50             ,8              ,27             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
12             ,135            ,True           ,True           ,51             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 147, True, 0.046, 50, 8, 27, 12, 135, True, True, 51
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (16h:38)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 27)       203715      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 8)        1216        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 135)       1018575     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 12)        1824        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 50, 35)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 675)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 51)           13311       concatenate_10[0][0]             
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 735)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 786)          0           phraseRnn[0][0]                  
                                                                 concatenate_11[0][0]             
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 147)          115689      concatenate_12[0][0]             
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 4)            592         dense_7[0][0]                    
==================================================================================================
Total params: 1,354,922
Trainable params: 1,354,922
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 279s - loss: 0.0654 - acc: 0.9829 - val_loss: 0.0558 - val_acc: 0.9856
Epoch 2/40
 - 263s - loss: 0.0495 - acc: 0.9879 - val_loss: 0.0551 - val_acc: 0.9859
Epoch 3/40
 - 272s - loss: 0.0463 - acc: 0.9888 - val_loss: 0.0591 - val_acc: 0.9857
Epoch 4/40
 - 263s - loss: 0.0450 - acc: 0.9892 - val_loss: 0.0620 - val_acc: 0.9859
Epoch 5/40
 - 264s - loss: 0.0441 - acc: 0.9895 - val_loss: 0.0663 - val_acc: 0.9854
Epoch 00005: early stopping
	TRAINING TIME: 23.8 minutes 
==================================================================================================
	PARSING TIME: 7.98 minutes 
==================================================================================================
	Identification : 0.582
	P, R  : 0.629, 0.542

==================================================================================================
	XP Ends: 25/6 (17 h:10)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 25/6 (17h:10)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 27)       195507      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 8)        1352        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 135)       977535      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 12)        2028        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 50, 35)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 675)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 51)           13311       concatenate_13[0][0]             
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 735)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 786)          0           phraseRnn[0][0]                  
                                                                 concatenate_14[0][0]             
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 147)          115689      concatenate_15[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 4)            592         dense_9[0][0]                    
==================================================================================================
Total params: 1,306,014
Trainable params: 1,306,014
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 178s - loss: 0.0733 - acc: 0.9800 - val_loss: 0.0604 - val_acc: 0.9838
Epoch 2/40
 - 178s - loss: 0.0521 - acc: 0.9868 - val_loss: 0.0606 - val_acc: 0.9846
Epoch 3/40
 - 178s - loss: 0.0484 - acc: 0.9881 - val_loss: 0.0632 - val_acc: 0.9846
Epoch 4/40
 - 178s - loss: 0.0467 - acc: 0.9885 - val_loss: 0.0687 - val_acc: 0.9845
Epoch 5/40
 - 179s - loss: 0.0459 - acc: 0.9888 - val_loss: 0.0741 - val_acc: 0.9845
Epoch 00005: early stopping
	TRAINING TIME: 15.83 minutes 
==================================================================================================
	PARSING TIME: 12.18 minutes 
==================================================================================================
	Identification : 0.532
	P, R  : 0.461, 0.629

==================================================================================================
	XP Ends: 25/6 (17 h:39)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (17h:39)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 27)       372411      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 8)        872         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 135)       1862055     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 12)        1308        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 50, 35)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 675)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 51)           13311       concatenate_16[0][0]             
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 735)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 786)          0           phraseRnn[0][0]                  
                                                                 concatenate_17[0][0]             
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 147)          115689      concatenate_18[0][0]             
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 4)            592         dense_11[0][0]                   
==================================================================================================
Total params: 2,366,238
Trainable params: 2,366,238
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 354s - loss: 0.0652 - acc: 0.9823 - val_loss: 0.0559 - val_acc: 0.9848
Epoch 2/40
 - 354s - loss: 0.0509 - acc: 0.9870 - val_loss: 0.0574 - val_acc: 0.9848
Epoch 3/40
 - 354s - loss: 0.0478 - acc: 0.9882 - val_loss: 0.0617 - val_acc: 0.9845
Epoch 4/40
 - 354s - loss: 0.0461 - acc: 0.9887 - val_loss: 0.0698 - val_acc: 0.9844
Epoch 5/40
 - 355s - loss: 0.0449 - acc: 0.9890 - val_loss: 0.0754 - val_acc: 0.9842
Epoch 00005: early stopping
	TRAINING TIME: 32.23 minutes 
==================================================================================================
	PARSING TIME: 5.37 minutes 
==================================================================================================
	Identification : 0.485
	P, R  : 0.459, 0.514

==================================================================================================
	XP Ends: 25/6 (18 h:17)
==================================================================================================
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpSuw0zi and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpYOYoma). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING:theano.gof.cmodule:The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpSuw0zi and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpYOYoma). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpSuw0zi and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpYOYoma). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING:theano.gof.cmodule:The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpSuw0zi and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpYOYoma). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpSuw0zi and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpYOYoma). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
WARNING:theano.gof.cmodule:The same cache key is associated to different modules (/home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpSuw0zi and /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/tmpYOYoma). This is not supposed to happen! You may need to manually delete your cache directory to fix this.
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,133            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.015          ,50             ,27             ,25             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
25             ,32             ,True           ,False          ,143            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 133, True, 0.015, 50, 27, 25, 25, 32, True, False, 143
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (18h:17)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 25)       188625      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 27)       4104        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 32)        241440      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 25)        3800        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 50, 52)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 128)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 100)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 143)          84084       concatenate_19[0][0]             
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 228)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 371)          0           phraseRnn[0][0]                  
                                                                 concatenate_20[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 133)          49476       concatenate_21[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 4)            536         dense_13[0][0]                   
==================================================================================================
Total params: 572,065
Trainable params: 572,065
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 298s - loss: 0.0710 - acc: 0.9816 - val_loss: 0.0606 - val_acc: 0.9848
Epoch 2/40
 - 298s - loss: 0.0547 - acc: 0.9862 - val_loss: 0.0587 - val_acc: 0.9851
Epoch 3/40
 - 298s - loss: 0.0510 - acc: 0.9873 - val_loss: 0.0594 - val_acc: 0.9851
Epoch 4/40
 - 298s - loss: 0.0488 - acc: 0.9880 - val_loss: 0.0609 - val_acc: 0.9853
Epoch 5/40
 - 298s - loss: 0.0476 - acc: 0.9884 - val_loss: 0.0636 - val_acc: 0.9850
Epoch 00005: early stopping
	TRAINING TIME: 26.63 minutes 
==================================================================================================
	PARSING TIME: 7.97 minutes 
==================================================================================================
	Identification : 0.594
	P, R  : 0.737, 0.497

==================================================================================================
	XP Ends: 25/6 (18 h:52)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 25/6 (18h:52)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 25)       181025      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 27)       4563        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 32)        231712      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 25)        4225        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 50, 52)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 128)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 100)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 143)          84084       concatenate_22[0][0]             
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 228)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 371)          0           phraseRnn[0][0]                  
                                                                 concatenate_23[0][0]             
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 133)          49476       concatenate_24[0][0]             
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 4)            536         dense_15[0][0]                   
==================================================================================================
Total params: 555,621
Trainable params: 555,621
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 202s - loss: 0.0828 - acc: 0.9775 - val_loss: 0.0674 - val_acc: 0.9821
Epoch 2/40
 - 202s - loss: 0.0602 - acc: 0.9843 - val_loss: 0.0647 - val_acc: 0.9830
Epoch 3/40
 - 202s - loss: 0.0553 - acc: 0.9859 - val_loss: 0.0654 - val_acc: 0.9831
Epoch 4/40
 - 202s - loss: 0.0526 - acc: 0.9867 - val_loss: 0.0692 - val_acc: 0.9826
Epoch 5/40
 - 202s - loss: 0.0507 - acc: 0.9873 - val_loss: 0.0704 - val_acc: 0.9826
Epoch 00005: early stopping
	TRAINING TIME: 17.83 minutes 
==================================================================================================
	PARSING TIME: 12.42 minutes 
==================================================================================================
	Identification : 0.426
	P, R  : 0.318, 0.646

==================================================================================================
	XP Ends: 25/6 (19 h:23)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (19h:23)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 25)       344825      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 27)       2943        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 32)        441376      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 25)        2725        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 50, 52)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 128)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 100)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 143)          84084       concatenate_25[0][0]             
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 228)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 371)          0           phraseRnn[0][0]                  
                                                                 concatenate_26[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 133)          49476       concatenate_27[0][0]             
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 4)            536         dense_17[0][0]                   
==================================================================================================
Total params: 925,965
Trainable params: 925,965
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 400s - loss: 0.0714 - acc: 0.9808 - val_loss: 0.0602 - val_acc: 0.9836
Epoch 2/40
 - 400s - loss: 0.0553 - acc: 0.9855 - val_loss: 0.0600 - val_acc: 0.9837
Epoch 3/40
 - 400s - loss: 0.0518 - acc: 0.9868 - val_loss: 0.0620 - val_acc: 0.9833
Epoch 4/40
 - 400s - loss: 0.0499 - acc: 0.9875 - val_loss: 0.0659 - val_acc: 0.9829
Epoch 5/40
 - 400s - loss: 0.0486 - acc: 0.9878 - val_loss: 0.0688 - val_acc: 0.9832
Epoch 00005: early stopping
	TRAINING TIME: 36.08 minutes 
==================================================================================================
	PARSING TIME: 5.4 minutes 
==================================================================================================
	Identification : 0.498
	P, R  : 0.467, 0.533

==================================================================================================
	XP Ends: 25/6 (20 h:5)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,212            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.013          ,50             ,5              ,32             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
38             ,71             ,True           ,True           ,63             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 212, True, 0.013, 50, 5, 32, 38, 71, True, True, 63
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (20h:5)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 32)       241440      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        760         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 71)        535695      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 38)        5776        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 50, 37)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 355)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 190)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 63)           19089       concatenate_28[0][0]             
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 545)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 608)          0           phraseRnn[0][0]                  
                                                                 concatenate_29[0][0]             
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 212)          129108      concatenate_30[0][0]             
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 4)            852         dense_19[0][0]                   
==================================================================================================
Total params: 932,720
Trainable params: 932,720
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 266s - loss: 0.0675 - acc: 0.9825 - val_loss: 0.0576 - val_acc: 0.9851
Epoch 2/40
 - 266s - loss: 0.0520 - acc: 0.9869 - val_loss: 0.0563 - val_acc: 0.9857
Epoch 3/40
 - 266s - loss: 0.0485 - acc: 0.9881 - val_loss: 0.0652 - val_acc: 0.9860
Epoch 4/40
 - 278s - loss: 0.0467 - acc: 0.9887 - val_loss: 0.0679 - val_acc: 0.9836
Epoch 5/40
 - 264s - loss: 0.0456 - acc: 0.9890 - val_loss: 0.0636 - val_acc: 0.9856
Epoch 00005: early stopping
	TRAINING TIME: 23.87 minutes 
==================================================================================================
	PARSING TIME: 8.0 minutes 
==================================================================================================
	Identification : 0.581
	P, R  : 0.676, 0.51

==================================================================================================
	XP Ends: 25/6 (20 h:37)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 25/6 (20h:37)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 32)       231712      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        845         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 71)        514111      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 38)        6422        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 50, 37)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 355)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 190)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 63)           19089       concatenate_31[0][0]             
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 545)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 608)          0           phraseRnn[0][0]                  
                                                                 concatenate_32[0][0]             
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 212)          129108      concatenate_33[0][0]             
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 4)            852         dense_21[0][0]                   
==================================================================================================
Total params: 902,139
Trainable params: 902,139
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 179s - loss: 0.0757 - acc: 0.9796 - val_loss: 0.0627 - val_acc: 0.9836
Epoch 2/40
 - 185s - loss: 0.0556 - acc: 0.9859 - val_loss: 0.0615 - val_acc: 0.9840
Epoch 3/40
 - 179s - loss: 0.0514 - acc: 0.9873 - val_loss: 0.0645 - val_acc: 0.9838
Epoch 4/40
 - 178s - loss: 0.0492 - acc: 0.9879 - val_loss: 0.0655 - val_acc: 0.9837
Epoch 5/40
 - 179s - loss: 0.0479 - acc: 0.9882 - val_loss: 0.0691 - val_acc: 0.9838
Epoch 00005: early stopping
	TRAINING TIME: 15.98 minutes 
==================================================================================================
	PARSING TIME: 12.28 minutes 
==================================================================================================
	Identification : 0.461
	P, R  : 0.371, 0.608

==================================================================================================
	XP Ends: 25/6 (21 h:6)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (21h:6)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 32)       441376      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        545         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 71)        979303      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 38)        4142        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 50, 37)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 355)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 190)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 63)           19089       concatenate_34[0][0]             
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 545)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 608)          0           phraseRnn[0][0]                  
                                                                 concatenate_35[0][0]             
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 212)          129108      concatenate_36[0][0]             
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 4)            852         dense_23[0][0]                   
==================================================================================================
Total params: 1,574,415
Trainable params: 1,574,415
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 357s - loss: 0.0685 - acc: 0.9814 - val_loss: 0.0585 - val_acc: 0.9842
Epoch 2/40
 - 355s - loss: 0.0531 - acc: 0.9864 - val_loss: 0.0584 - val_acc: 0.9841
Epoch 3/40
 - 364s - loss: 0.0498 - acc: 0.9875 - val_loss: 0.0613 - val_acc: 0.9838
Epoch 4/40
 - 355s - loss: 0.0479 - acc: 0.9882 - val_loss: 0.0649 - val_acc: 0.9838
Epoch 5/40
 - 357s - loss: 0.0467 - acc: 0.9885 - val_loss: 0.0690 - val_acc: 0.9835
Epoch 00005: early stopping
	TRAINING TIME: 32.7 minutes 
==================================================================================================
	PARSING TIME: 6.13 minutes 
==================================================================================================
	Identification : 0.493
	P, R  : 0.434, 0.571

==================================================================================================
	XP Ends: 25/6 (21 h:45)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,216            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.021          ,50             ,7              ,81             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
12             ,39             ,True           ,True           ,87             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 216, True, 0.021, 50, 7, 81, 12, 39, True, True, 87
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (21h:45)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 81)       926478      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        1064        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 39)        446082      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 12)        1824        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 50, 88)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 195)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 87)           45936       concatenate_37[0][0]             
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 255)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 342)          0           phraseRnn[0][0]                  
                                                                 concatenate_38[0][0]             
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 216)          74088       concatenate_39[0][0]             
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 4)            868         dense_25[0][0]                   
==================================================================================================
Total params: 1,496,340
Trainable params: 1,496,340
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 282s - loss: 0.0664 - acc: 0.9824 - val_loss: 0.0538 - val_acc: 0.9863
Epoch 2/40
 - 292s - loss: 0.0493 - acc: 0.9880 - val_loss: 0.0536 - val_acc: 0.9864
Epoch 3/40
 - 298s - loss: 0.0464 - acc: 0.9889 - val_loss: 0.0557 - val_acc: 0.9864
Epoch 4/40
 - 283s - loss: 0.0452 - acc: 0.9892 - val_loss: 0.0589 - val_acc: 0.9864
Epoch 5/40
 - 282s - loss: 0.0444 - acc: 0.9893 - val_loss: 0.0639 - val_acc: 0.9863
Epoch 00005: early stopping
	TRAINING TIME: 25.48 minutes 
==================================================================================================
	PARSING TIME: 8.07 minutes 
==================================================================================================
	Identification : 0.552
	P, R  : 0.532, 0.573

==================================================================================================
	XP Ends: 25/6 (22 h:19)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 25/6 (22h:19)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 81)       761481      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        1183        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 39)        366639      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 12)        2028        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 50, 88)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 195)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 87)           45936       concatenate_40[0][0]             
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 255)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 342)          0           phraseRnn[0][0]                  
                                                                 concatenate_41[0][0]             
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 216)          74088       concatenate_42[0][0]             
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 4)            868         dense_27[0][0]                   
==================================================================================================
Total params: 1,252,223
Trainable params: 1,252,223
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 185s - loss: 0.0748 - acc: 0.9794 - val_loss: 0.0575 - val_acc: 0.9845
Epoch 2/40
 - 184s - loss: 0.0527 - acc: 0.9868 - val_loss: 0.0576 - val_acc: 0.9851
Epoch 3/40
 - 183s - loss: 0.0491 - acc: 0.9879 - val_loss: 0.0604 - val_acc: 0.9853
Epoch 4/40
 - 183s - loss: 0.0474 - acc: 0.9884 - val_loss: 0.0666 - val_acc: 0.9851
Epoch 5/40
 - 184s - loss: 0.0463 - acc: 0.9885 - val_loss: 0.0753 - val_acc: 0.9848
Epoch 00005: early stopping
	TRAINING TIME: 16.4 minutes 
==================================================================================================
	PARSING TIME: 12.32 minutes 
==================================================================================================
	Identification : 0.456
	P, R  : 0.364, 0.609

==================================================================================================
	XP Ends: 25/6 (22 h:48)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (22h:48)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 81)       1787751     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        763         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 39)        860769      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 12)        1308        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 50, 88)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 195)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 87)           45936       concatenate_43[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 255)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 342)          0           phraseRnn[0][0]                  
                                                                 concatenate_44[0][0]             
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 216)          74088       concatenate_45[0][0]             
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 4)            868         dense_29[0][0]                   
==================================================================================================
Total params: 2,771,483
Trainable params: 2,771,483
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 403s - loss: 0.0640 - acc: 0.9829 - val_loss: 0.0576 - val_acc: 0.9847
Epoch 2/40
 - 380s - loss: 0.0490 - acc: 0.9878 - val_loss: 0.0591 - val_acc: 0.9846
Epoch 3/40
 - 380s - loss: 0.0463 - acc: 0.9888 - val_loss: 0.0624 - val_acc: 0.9845
Epoch 4/40
 - 387s - loss: 0.0447 - acc: 0.9892 - val_loss: 0.0712 - val_acc: 0.9842
Epoch 5/40
 - 380s - loss: 0.0434 - acc: 0.9893 - val_loss: 0.0810 - val_acc: 0.9840
Epoch 00005: early stopping
	TRAINING TIME: 35.15 minutes 
==================================================================================================
	PARSING TIME: 5.32 minutes 
==================================================================================================
	Identification : 0.497
	P, R  : 0.461, 0.539

==================================================================================================
	XP Ends: 25/6 (23 h:29)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,111            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.036          ,50             ,16             ,153            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
32             ,66             ,False          ,True           ,52             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 111, True, 0.036, 50, 16, 153, 32, 66, False, True, 52
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 25/6 (23h:29)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 153)      1154385     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 16)       2432        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 66)        497970      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 32)        4864        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 50, 169)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 264)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 128)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 52)           34632       concatenate_46[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 392)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 444)          0           phraseRnn[0][0]                  
                                                                 concatenate_47[0][0]             
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 111)          49395       concatenate_48[0][0]             
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 4)            448         dense_31[0][0]                   
==================================================================================================
Total params: 1,744,126
Trainable params: 1,744,126
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 284s - loss: 0.0640 - acc: 0.9833 - val_loss: 0.0551 - val_acc: 0.9861
Epoch 2/40
 - 283s - loss: 0.0501 - acc: 0.9875 - val_loss: 0.0543 - val_acc: 0.9864
Epoch 3/40
 - 272s - loss: 0.0470 - acc: 0.9885 - val_loss: 0.0574 - val_acc: 0.9861
Epoch 4/40
 - 271s - loss: 0.0453 - acc: 0.9891 - val_loss: 0.0656 - val_acc: 0.9862
Epoch 5/40
 - 272s - loss: 0.0444 - acc: 0.9893 - val_loss: 0.0653 - val_acc: 0.9861
Epoch 00005: early stopping
	TRAINING TIME: 24.62 minutes 
==================================================================================================
	PARSING TIME: 7.87 minutes 
==================================================================================================
	Identification : 0.575
	P, R  : 0.65, 0.516

==================================================================================================
	XP Ends: 26/6 (0 h:2)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (0h:2)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 153)      1107873     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 16)       2704        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 66)        477906      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 32)        5408        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 50, 169)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 264)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 128)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 52)           34632       concatenate_49[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 392)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 444)          0           phraseRnn[0][0]                  
                                                                 concatenate_50[0][0]             
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 111)          49395       concatenate_51[0][0]             
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 4)            448         dense_33[0][0]                   
==================================================================================================
Total params: 1,678,366
Trainable params: 1,678,366
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 192s - loss: 0.0719 - acc: 0.9806 - val_loss: 0.0591 - val_acc: 0.9848
Epoch 2/40
 - 192s - loss: 0.0523 - acc: 0.9868 - val_loss: 0.0598 - val_acc: 0.9847
Epoch 3/40
 - 184s - loss: 0.0488 - acc: 0.9881 - val_loss: 0.0613 - val_acc: 0.9849
Epoch 4/40
 - 184s - loss: 0.0473 - acc: 0.9884 - val_loss: 0.0684 - val_acc: 0.9847
Epoch 5/40
 - 184s - loss: 0.0464 - acc: 0.9886 - val_loss: 0.0723 - val_acc: 0.9844
Epoch 00005: early stopping
	TRAINING TIME: 16.58 minutes 
==================================================================================================
	PARSING TIME: 12.3 minutes 
==================================================================================================
	Identification : 0.477
	P, R  : 0.378, 0.647

==================================================================================================
	XP Ends: 26/6 (0 h:32)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (0h:32)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 153)      2110329     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 16)       1744        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 66)        910338      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 32)        3488        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 50, 169)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 264)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 128)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 52)           34632       concatenate_52[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 392)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 444)          0           phraseRnn[0][0]                  
                                                                 concatenate_53[0][0]             
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 111)          49395       concatenate_54[0][0]             
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 4)            448         dense_35[0][0]                   
==================================================================================================
Total params: 3,110,374
Trainable params: 3,110,374
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 368s - loss: 0.0657 - acc: 0.9821 - val_loss: 0.0569 - val_acc: 0.9843
Epoch 2/40
 - 367s - loss: 0.0523 - acc: 0.9865 - val_loss: 0.0576 - val_acc: 0.9844
Epoch 3/40
 - 367s - loss: 0.0494 - acc: 0.9874 - val_loss: 0.0613 - val_acc: 0.9841
Epoch 4/40
 - 367s - loss: 0.0474 - acc: 0.9882 - val_loss: 0.0671 - val_acc: 0.9839
Epoch 5/40
 - 367s - loss: 0.0460 - acc: 0.9886 - val_loss: 0.0777 - val_acc: 0.9839
Epoch 00005: early stopping
	TRAINING TIME: 33.4 minutes 
==================================================================================================
	PARSING TIME: 5.42 minutes 
==================================================================================================
	Identification : 0.486
	P, R  : 0.448, 0.531

==================================================================================================
	XP Ends: 26/6 (1 h:11)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,25             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.055          ,50             ,24             ,60             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
33             ,149            ,True           ,True           ,35             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 25, True, 0.055, 50, 24, 60, 33, 149, True, True, 35
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (1h:11)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 60)       686280      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 24)       3648        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 149)       1704262     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 33)        5016        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 50, 84)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 745)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 165)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 35)           12600       concatenate_55[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 910)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 945)          0           phraseRnn[0][0]                  
                                                                 concatenate_56[0][0]             
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 25)           23650       concatenate_57[0][0]             
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 4)            104         dense_37[0][0]                   
==================================================================================================
Total params: 2,435,560
Trainable params: 2,435,560
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 296s - loss: 0.0637 - acc: 0.9832 - val_loss: 0.0537 - val_acc: 0.9864
Epoch 2/40
 - 296s - loss: 0.0484 - acc: 0.9881 - val_loss: 0.0544 - val_acc: 0.9865
Epoch 3/40
 - 266s - loss: 0.0460 - acc: 0.9889 - val_loss: 0.0554 - val_acc: 0.9866
Epoch 4/40
 - 266s - loss: 0.0449 - acc: 0.9893 - val_loss: 0.0589 - val_acc: 0.9863
Epoch 5/40
 - 266s - loss: 0.0443 - acc: 0.9894 - val_loss: 0.0617 - val_acc: 0.9863
Epoch 00005: early stopping
	TRAINING TIME: 24.73 minutes 
==================================================================================================
	PARSING TIME: 8.12 minutes 
==================================================================================================
	Identification : 0.532
	P, R  : 0.471, 0.612

==================================================================================================
	XP Ends: 26/6 (1 h:44)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (1h:44)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 60)       564060      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 24)       4056        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 149)       1400749     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 33)        5577        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 50, 84)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 745)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 165)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 35)           12600       concatenate_58[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 910)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 945)          0           phraseRnn[0][0]                  
                                                                 concatenate_59[0][0]             
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 25)           23650       concatenate_60[0][0]             
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 4)            104         dense_39[0][0]                   
==================================================================================================
Total params: 2,010,796
Trainable params: 2,010,796
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 181s - loss: 0.0730 - acc: 0.9801 - val_loss: 0.0591 - val_acc: 0.9842
Epoch 2/40
 - 179s - loss: 0.0516 - acc: 0.9870 - val_loss: 0.0577 - val_acc: 0.9851
Epoch 3/40
 - 179s - loss: 0.0483 - acc: 0.9882 - val_loss: 0.0602 - val_acc: 0.9852
Epoch 4/40
 - 179s - loss: 0.0470 - acc: 0.9885 - val_loss: 0.0638 - val_acc: 0.9850
Epoch 5/40
 - 180s - loss: 0.0464 - acc: 0.9886 - val_loss: 0.0672 - val_acc: 0.9851
Epoch 00005: early stopping
	TRAINING TIME: 15.98 minutes 
==================================================================================================
	PARSING TIME: 12.18 minutes 
==================================================================================================
	Identification : 0.48
	P, R  : 0.398, 0.604

==================================================================================================
	XP Ends: 26/6 (2 h:13)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (2h:13)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 60)       1324260     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 24)       2616        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 149)       3288579     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 33)        3597        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 50, 84)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 745)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 165)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 35)           12600       concatenate_61[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 910)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 945)          0           phraseRnn[0][0]                  
                                                                 concatenate_62[0][0]             
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 25)           23650       concatenate_63[0][0]             
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 4)            104         dense_41[0][0]                   
==================================================================================================
Total params: 4,655,406
Trainable params: 4,655,406
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 366s - loss: 0.0626 - acc: 0.9833 - val_loss: 0.0577 - val_acc: 0.9843
Epoch 2/40
 - 403s - loss: 0.0485 - acc: 0.9880 - val_loss: 0.0597 - val_acc: 0.9843
Epoch 3/40
 - 360s - loss: 0.0460 - acc: 0.9889 - val_loss: 0.0631 - val_acc: 0.9843
Epoch 4/40
 - 362s - loss: 0.0448 - acc: 0.9892 - val_loss: 0.0679 - val_acc: 0.9843
Epoch 5/40
 - 360s - loss: 0.0438 - acc: 0.9894 - val_loss: 0.0784 - val_acc: 0.9840
Epoch 00005: early stopping
	TRAINING TIME: 33.8 minutes 
==================================================================================================
	PARSING TIME: 5.38 minutes 
==================================================================================================
	Identification : 0.48
	P, R  : 0.405, 0.59

==================================================================================================
	XP Ends: 26/6 (2 h:53)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,27             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.078          ,50             ,8              ,39             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
5              ,43             ,False          ,False          ,37             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 27, True, 0.078, 50, 8, 39, 5, 43, False, False, 37
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (2h:53)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 3)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 3)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 39)       446082      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 8)        1216        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 3, 43)        491834      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 3, 5)         760         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 50, 47)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 129)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 15)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 37)           9435        concatenate_64[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 144)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 181)          0           phraseRnn[0][0]                  
                                                                 concatenate_65[0][0]             
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 27)           4914        concatenate_66[0][0]             
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 4)            112         dense_43[0][0]                   
==================================================================================================
Total params: 954,353
Trainable params: 954,353
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 265s - loss: 0.0664 - acc: 0.9826 - val_loss: 0.0547 - val_acc: 0.9856
Epoch 2/40
 - 265s - loss: 0.0507 - acc: 0.9873 - val_loss: 0.0557 - val_acc: 0.9861
Epoch 3/40
 - 265s - loss: 0.0477 - acc: 0.9884 - val_loss: 0.0575 - val_acc: 0.9855
Epoch 4/40
 - 266s - loss: 0.0462 - acc: 0.9888 - val_loss: 0.0614 - val_acc: 0.9851
Epoch 5/40
 - 266s - loss: 0.0452 - acc: 0.9891 - val_loss: 0.0641 - val_acc: 0.9857
Epoch 00005: early stopping
	TRAINING TIME: 23.57 minutes 
==================================================================================================
	PARSING TIME: 8.15 minutes 
==================================================================================================
	Identification : 0.495
	P, R  : 0.717, 0.378

==================================================================================================
	XP Ends: 26/6 (3 h:25)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (3h:25)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 3)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 3)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 39)       366639      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 8)        1352        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 3, 43)        404243      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 3, 5)         845         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 50, 47)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 129)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 15)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 37)           9435        concatenate_67[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 144)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 181)          0           phraseRnn[0][0]                  
                                                                 concatenate_68[0][0]             
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 27)           4914        concatenate_69[0][0]             
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 4)            112         dense_45[0][0]                   
==================================================================================================
Total params: 787,540
Trainable params: 787,540
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 182s - loss: 0.0765 - acc: 0.9790 - val_loss: 0.0625 - val_acc: 0.9824
Epoch 2/40
 - 180s - loss: 0.0551 - acc: 0.9858 - val_loss: 0.0616 - val_acc: 0.9839
Epoch 3/40
 - 180s - loss: 0.0511 - acc: 0.9871 - val_loss: 0.0648 - val_acc: 0.9833
Epoch 4/40
 - 180s - loss: 0.0491 - acc: 0.9878 - val_loss: 0.0693 - val_acc: 0.9833
Epoch 5/40
 - 180s - loss: 0.0478 - acc: 0.9881 - val_loss: 0.0745 - val_acc: 0.9834
Epoch 00005: early stopping
	TRAINING TIME: 15.95 minutes 
==================================================================================================
	PARSING TIME: 12.42 minutes 
==================================================================================================
	Identification : 0.42
	P, R  : 0.334, 0.564

==================================================================================================
	XP Ends: 26/6 (3 h:54)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (3h:54)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 3)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 3)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 39)       860769      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 8)        872         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 3, 43)        949053      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 3, 5)         545         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 50, 47)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 129)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 15)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 37)           9435        concatenate_70[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 144)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 181)          0           phraseRnn[0][0]                  
                                                                 concatenate_71[0][0]             
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 27)           4914        concatenate_72[0][0]             
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 4)            112         dense_47[0][0]                   
==================================================================================================
Total params: 1,825,700
Trainable params: 1,825,700
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 360s - loss: 0.0643 - acc: 0.9824 - val_loss: 0.0599 - val_acc: 0.9837
Epoch 2/40
 - 356s - loss: 0.0511 - acc: 0.9868 - val_loss: 0.0602 - val_acc: 0.9837
Epoch 3/40
 - 355s - loss: 0.0486 - acc: 0.9877 - val_loss: 0.0635 - val_acc: 0.9834
Epoch 4/40
 - 353s - loss: 0.0469 - acc: 0.9883 - val_loss: 0.0699 - val_acc: 0.9837
Epoch 5/40
 - 355s - loss: 0.0452 - acc: 0.9887 - val_loss: 0.0883 - val_acc: 0.9833
Epoch 00005: early stopping
	TRAINING TIME: 32.42 minutes 
==================================================================================================
	PARSING TIME: 5.43 minutes 
==================================================================================================
	Identification : 0.48
	P, R  : 0.432, 0.539

==================================================================================================
	XP Ends: 26/6 (4 h:32)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,96             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.117          ,50             ,22             ,76             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
8              ,177            ,True           ,True           ,30             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 96, True, 0.117, 50, 22, 76, 8, 177, True, True, 30
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (4h:32)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 76)       573420      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 22)       3344        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 177)       1335465     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 8)         1216        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 50, 98)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 885)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 40)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 30)           11610       concatenate_73[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 925)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 955)          0           phraseRnn[0][0]                  
                                                                 concatenate_74[0][0]             
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 96)           91776       concatenate_75[0][0]             
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 4)            388         dense_49[0][0]                   
==================================================================================================
Total params: 2,017,219
Trainable params: 2,017,219
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 266s - loss: 0.0757 - acc: 0.9815 - val_loss: 0.0563 - val_acc: 0.9855
Epoch 2/40
 - 265s - loss: 0.0507 - acc: 0.9875 - val_loss: 0.0565 - val_acc: 0.9856
Epoch 3/40
 - 265s - loss: 0.0474 - acc: 0.9885 - val_loss: 0.0601 - val_acc: 0.9856
Epoch 4/40
 - 264s - loss: 0.0458 - acc: 0.9889 - val_loss: 0.0618 - val_acc: 0.9856
Epoch 5/40
 - 265s - loss: 0.0448 - acc: 0.9892 - val_loss: 0.0670 - val_acc: 0.9854
Epoch 00005: early stopping
	TRAINING TIME: 23.58 minutes 
==================================================================================================
	PARSING TIME: 8.0 minutes 
==================================================================================================
	Identification : 0.59
	P, R  : 0.656, 0.536

==================================================================================================
	XP Ends: 26/6 (5 h:4)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (5h:4)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 76)       550316      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 22)       3718        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 177)       1281657     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 8)         1352        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 50, 98)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 885)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 40)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 30)           11610       concatenate_76[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 925)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 955)          0           phraseRnn[0][0]                  
                                                                 concatenate_77[0][0]             
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 96)           91776       concatenate_78[0][0]             
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 4)            388         dense_51[0][0]                   
==================================================================================================
Total params: 1,940,817
Trainable params: 1,940,817
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 179s - loss: 1.2029 - acc: 0.9103 - val_loss: 0.0639 - val_acc: 0.9833
Epoch 2/40
 - 179s - loss: 0.0553 - acc: 0.9859 - val_loss: 0.0612 - val_acc: 0.9844
Epoch 3/40
 - 179s - loss: 0.0498 - acc: 0.9876 - val_loss: 0.0630 - val_acc: 0.9843
Epoch 4/40
 - 179s - loss: 0.0480 - acc: 0.9882 - val_loss: 0.0666 - val_acc: 0.9844
Epoch 5/40
 - 179s - loss: 0.0469 - acc: 0.9885 - val_loss: 0.0698 - val_acc: 0.9843
Epoch 00005: early stopping
	TRAINING TIME: 15.95 minutes 
==================================================================================================
	PARSING TIME: 13.15 minutes 
==================================================================================================
	Identification : 0.556
	P, R  : 0.492, 0.64

==================================================================================================
	XP Ends: 26/6 (5 h:34)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (5h:34)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 76)       1048268     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 22)       2398        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 177)       2441361     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 8)         872         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 50, 98)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 885)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 40)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 30)           11610       concatenate_79[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 925)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 955)          0           phraseRnn[0][0]                  
                                                                 concatenate_80[0][0]             
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 96)           91776       concatenate_81[0][0]             
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 4)            388         dense_53[0][0]                   
==================================================================================================
Total params: 3,596,673
Trainable params: 3,596,673
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 399s - loss: 0.0739 - acc: 0.9808 - val_loss: 0.0610 - val_acc: 0.9842
Epoch 2/40
 - 389s - loss: 0.0519 - acc: 0.9867 - val_loss: 0.0597 - val_acc: 0.9842
Epoch 3/40
 - 361s - loss: 0.0489 - acc: 0.9878 - val_loss: 0.0617 - val_acc: 0.9840
Epoch 4/40
 - 358s - loss: 0.0471 - acc: 0.9884 - val_loss: 0.0674 - val_acc: 0.9839
Epoch 5/40
 - 359s - loss: 0.0460 - acc: 0.9888 - val_loss: 0.0726 - val_acc: 0.9838
Epoch 00005: early stopping
	TRAINING TIME: 34.08 minutes 
==================================================================================================
	PARSING TIME: 5.4 minutes 
==================================================================================================
	Identification : 0.483
	P, R  : 0.44, 0.535

==================================================================================================
	XP Ends: 26/6 (6 h:14)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,43             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.013          ,50             ,7              ,142            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
14             ,63             ,False          ,True           ,403            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 43, True, 0.013, 50, 7, 142, 14, 63, False, True, 403
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (6h:14)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 142)      1071390     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        1064        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 63)        475335      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 14)        2128        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 50, 149)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 252)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 56)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 403)          668577      concatenate_82[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 308)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 711)          0           phraseRnn[0][0]                  
                                                                 concatenate_83[0][0]             
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 43)           30616       concatenate_84[0][0]             
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 4)            176         dense_55[0][0]                   
==================================================================================================
Total params: 2,249,286
Trainable params: 2,249,286
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 573s - loss: 0.0953 - acc: 0.9750 - val_loss: 0.0743 - val_acc: 0.9810
Epoch 2/40
 - 573s - loss: 0.0610 - acc: 0.9843 - val_loss: 0.0666 - val_acc: 0.9836
Epoch 3/40
 - 573s - loss: 0.0556 - acc: 0.9860 - val_loss: 0.0657 - val_acc: 0.9844
Epoch 4/40
 - 572s - loss: 0.0523 - acc: 0.9869 - val_loss: 0.0661 - val_acc: 0.9844
Epoch 5/40
 - 573s - loss: 0.0500 - acc: 0.9877 - val_loss: 0.0675 - val_acc: 0.9845
Epoch 00005: early stopping
	TRAINING TIME: 49.32 minutes 
==================================================================================================
	PARSING TIME: 8.82 minutes 
==================================================================================================
	Identification : 0.581
	P, R  : 0.747, 0.476

==================================================================================================
	XP Ends: 26/6 (7 h:12)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (7h:12)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 142)      1028222     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        1183        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 63)        456183      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 14)        2366        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 50, 149)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 252)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 56)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 403)          668577      concatenate_85[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 308)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 711)          0           phraseRnn[0][0]                  
                                                                 concatenate_86[0][0]             
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 43)           30616       concatenate_87[0][0]             
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 4)            176         dense_57[0][0]                   
==================================================================================================
Total params: 2,187,323
Trainable params: 2,187,323
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 388s - loss: 0.0979 - acc: 0.9736 - val_loss: 0.0718 - val_acc: 0.9817
Epoch 2/40
 - 388s - loss: 0.0617 - acc: 0.9840 - val_loss: 0.0716 - val_acc: 0.9814
Epoch 3/40
 - 388s - loss: 0.0560 - acc: 0.9857 - val_loss: 0.0713 - val_acc: 0.9822
Epoch 4/40
 - 406s - loss: 0.0529 - acc: 0.9866 - val_loss: 0.0713 - val_acc: 0.9833
Epoch 5/40
 - 388s - loss: 0.0511 - acc: 0.9873 - val_loss: 0.0738 - val_acc: 0.9827
Epoch 00005: early stopping
	TRAINING TIME: 33.63 minutes 
==================================================================================================
	PARSING TIME: 13.87 minutes 
==================================================================================================
	Identification : 0.467
	P, R  : 0.363, 0.656

==================================================================================================
	XP Ends: 26/6 (8 h:0)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (8h:0)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 142)      1958606     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        763         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 63)        868959      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 14)        1526        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 50, 149)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 252)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 56)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 403)          668577      concatenate_88[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 308)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 711)          0           phraseRnn[0][0]                  
                                                                 concatenate_89[0][0]             
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 43)           30616       concatenate_90[0][0]             
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 4)            176         dense_59[0][0]                   
==================================================================================================
Total params: 3,529,223
Trainable params: 3,529,223
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 776s - loss: 0.0777 - acc: 0.9793 - val_loss: 0.0628 - val_acc: 0.9830
Epoch 2/40
 - 777s - loss: 0.0565 - acc: 0.9853 - val_loss: 0.0616 - val_acc: 0.9836
Epoch 3/40
 - 776s - loss: 0.0527 - acc: 0.9866 - val_loss: 0.0635 - val_acc: 0.9835
Epoch 4/40
 - 815s - loss: 0.0506 - acc: 0.9872 - val_loss: 0.0661 - val_acc: 0.9833
Epoch 5/40
 - 815s - loss: 0.0493 - acc: 0.9877 - val_loss: 0.0692 - val_acc: 0.9831
Epoch 00005: early stopping
	TRAINING TIME: 68.92 minutes 
==================================================================================================
	PARSING TIME: 5.92 minutes 
==================================================================================================
	Identification : 0.466
	P, R  : 0.518, 0.424

==================================================================================================
	XP Ends: 26/6 (9 h:16)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,105            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.015          ,50             ,28             ,40             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
20             ,190            ,True           ,False          ,482            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 105, True, 0.015, 50, 28, 40, 20, 190, True, False, 482
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (9h:16)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 40)       301800      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 28)       4256        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 190)       1433550     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 20)        3040        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 50, 68)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 760)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 80)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 482)          796746      concatenate_91[0][0]             
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 840)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 1322)         0           phraseRnn[0][0]                  
                                                                 concatenate_92[0][0]             
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 105)          138915      concatenate_93[0][0]             
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 4)            424         dense_61[0][0]                   
==================================================================================================
Total params: 2,678,731
Trainable params: 2,678,731
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 659s - loss: 3.5875 - acc: 0.7617 - val_loss: 3.7768 - val_acc: 0.7536
Epoch 2/40
 - 659s - loss: 0.2820 - acc: 0.9706 - val_loss: 0.0693 - val_acc: 0.9837
Epoch 3/40
 - 654s - loss: 0.0550 - acc: 0.9863 - val_loss: 0.0684 - val_acc: 0.9843
Epoch 4/40
 - 660s - loss: 0.0528 - acc: 0.9869 - val_loss: 0.0689 - val_acc: 0.9843
Epoch 5/40
 - 659s - loss: 0.0512 - acc: 0.9873 - val_loss: 0.0702 - val_acc: 0.9844
Epoch 6/40
 - 630s - loss: 0.0498 - acc: 0.9877 - val_loss: 0.0705 - val_acc: 0.9843
Epoch 00006: early stopping
	TRAINING TIME: 66.95 minutes 
==================================================================================================
	PARSING TIME: 9.48 minutes 
==================================================================================================
	Identification : 0.595
	P, R  : 0.755, 0.491

==================================================================================================
	XP Ends: 26/6 (10 h:33)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (10h:33)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 40)       289640      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 28)       4732        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 190)       1375790     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 20)        3380        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 50, 68)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 760)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 80)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 482)          796746      concatenate_94[0][0]             
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 840)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 1322)         0           phraseRnn[0][0]                  
                                                                 concatenate_95[0][0]             
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 105)          138915      concatenate_96[0][0]             
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 4)            424         dense_63[0][0]                   
==================================================================================================
Total params: 2,609,627
Trainable params: 2,609,627
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 447s - loss: 0.1149 - acc: 0.9717 - val_loss: 0.0751 - val_acc: 0.9812
Epoch 2/40
 - 447s - loss: 0.0635 - acc: 0.9835 - val_loss: 0.2415 - val_acc: 0.8997
Epoch 3/40
 - 446s - loss: 0.0582 - acc: 0.9849 - val_loss: 0.0748 - val_acc: 0.9819
Epoch 4/40
 - 447s - loss: 0.0550 - acc: 0.9860 - val_loss: 0.0728 - val_acc: 0.9823
Epoch 5/40
 - 447s - loss: 0.0529 - acc: 0.9865 - val_loss: 0.0761 - val_acc: 0.9820
Epoch 00005: early stopping
	TRAINING TIME: 38.23 minutes 
==================================================================================================
	PARSING TIME: 15.68 minutes 
==================================================================================================
	Identification : 0.465
	P, R  : 0.365, 0.64

==================================================================================================
	XP Ends: 26/6 (11 h:27)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (11h:27)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 40)       551720      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 28)       3052        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 190)       2620670     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 20)        2180        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 50, 68)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 760)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 80)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 482)          796746      concatenate_97[0][0]             
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 840)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 1322)         0           phraseRnn[0][0]                  
                                                                 concatenate_98[0][0]             
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 105)          138915      concatenate_99[0][0]             
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 4)            424         dense_65[0][0]                   
==================================================================================================
Total params: 4,113,707
Trainable params: 4,113,707
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 891s - loss: 0.0864 - acc: 0.9772 - val_loss: 0.0650 - val_acc: 0.9827
Epoch 2/40
 - 891s - loss: 0.0580 - acc: 0.9847 - val_loss: 0.0624 - val_acc: 0.9835
Epoch 3/40
 - 891s - loss: 0.0546 - acc: 0.9858 - val_loss: 0.0633 - val_acc: 0.9832
Epoch 4/40
 - 890s - loss: 0.0528 - acc: 0.9863 - val_loss: 0.0648 - val_acc: 0.9833
Epoch 5/40
 - 890s - loss: 0.0514 - acc: 0.9867 - val_loss: 0.0672 - val_acc: 0.9833
Epoch 00005: early stopping
	TRAINING TIME: 77.15 minutes 
==================================================================================================
	PARSING TIME: 6.75 minutes 
==================================================================================================
	Identification : 0.498
	P, R  : 0.516, 0.482

==================================================================================================
	XP Ends: 26/6 (12 h:51)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,52             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.022          ,50             ,33             ,33             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
5              ,28             ,True           ,False          ,224            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 52, True, 0.022, 50, 33, 33, 5, 28, True, False, 224
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (12h:51)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 33)       248985      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 33)       5016        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 28)        211260      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 5)         760         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 50, 66)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 112)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 20)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 224)          195552      concatenate_100[0][0]            
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 132)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 356)          0           phraseRnn[0][0]                  
                                                                 concatenate_101[0][0]            
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 52)           18564       concatenate_102[0][0]            
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 4)            212         dense_67[0][0]                   
==================================================================================================
Total params: 680,349
Trainable params: 680,349
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 379s - loss: 0.0750 - acc: 0.9802 - val_loss: 0.0611 - val_acc: 0.9841
Epoch 2/40
 - 380s - loss: 0.0548 - acc: 0.9862 - val_loss: 0.0600 - val_acc: 0.9852
Epoch 3/40
 - 379s - loss: 0.0509 - acc: 0.9875 - val_loss: 0.0618 - val_acc: 0.9844
Epoch 4/40
 - 379s - loss: 0.0488 - acc: 0.9881 - val_loss: 0.0623 - val_acc: 0.9850
Epoch 5/40
 - 378s - loss: 0.0473 - acc: 0.9885 - val_loss: 0.0655 - val_acc: 0.9851
Epoch 00005: early stopping
	TRAINING TIME: 33.18 minutes 
==================================================================================================
	PARSING TIME: 7.92 minutes 
==================================================================================================
	Identification : 0.593
	P, R  : 0.779, 0.479

==================================================================================================
	XP Ends: 26/6 (13 h:33)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (13h:33)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 33)       238953      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 33)       5577        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 28)        202748      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 5)         845         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 50, 66)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 112)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 20)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 224)          195552      concatenate_103[0][0]            
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 132)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 356)          0           phraseRnn[0][0]                  
                                                                 concatenate_104[0][0]            
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 52)           18564       concatenate_105[0][0]            
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 4)            212         dense_69[0][0]                   
==================================================================================================
Total params: 662,451
Trainable params: 662,451
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 252s - loss: 0.0851 - acc: 0.9768 - val_loss: 0.0668 - val_acc: 0.9816
Epoch 2/40
 - 252s - loss: 0.0601 - acc: 0.9843 - val_loss: 0.0649 - val_acc: 0.9828
Epoch 3/40
 - 240s - loss: 0.0547 - acc: 0.9860 - val_loss: 0.0658 - val_acc: 0.9831
Epoch 4/40
 - 240s - loss: 0.0521 - acc: 0.9868 - val_loss: 0.0686 - val_acc: 0.9829
Epoch 5/40
 - 240s - loss: 0.0504 - acc: 0.9874 - val_loss: 0.0697 - val_acc: 0.9828
Epoch 00005: early stopping
	TRAINING TIME: 21.38 minutes 
==================================================================================================
	PARSING TIME: 12.53 minutes 
==================================================================================================
	Identification : 0.458
	P, R  : 0.37, 0.602

==================================================================================================
	XP Ends: 26/6 (14 h:7)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (14h:7)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 33)       455169      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 33)       3597        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 28)        386204      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 5)         545         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 50, 66)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 112)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 20)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 224)          195552      concatenate_106[0][0]            
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 132)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 356)          0           phraseRnn[0][0]                  
                                                                 concatenate_107[0][0]            
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 52)           18564       concatenate_108[0][0]            
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 4)            212         dense_71[0][0]                   
==================================================================================================
Total params: 1,059,843
Trainable params: 1,059,843
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 476s - loss: 0.0899 - acc: 0.9758 - val_loss: 0.0711 - val_acc: 0.9810
Epoch 2/40
 - 477s - loss: 0.0599 - acc: 0.9842 - val_loss: 0.0640 - val_acc: 0.9830
Epoch 3/40
 - 476s - loss: 0.0560 - acc: 0.9852 - val_loss: 0.0649 - val_acc: 0.9833
Epoch 4/40
 - 477s - loss: 0.0534 - acc: 0.9861 - val_loss: 0.0662 - val_acc: 0.9832
Epoch 5/40
 - 477s - loss: 0.0515 - acc: 0.9867 - val_loss: 0.0678 - val_acc: 0.9832
Epoch 00005: early stopping
	TRAINING TIME: 42.7 minutes 
==================================================================================================
	PARSING TIME: 5.42 minutes 
==================================================================================================
	Identification : 0.455
	P, R  : 0.587, 0.371

==================================================================================================
	XP Ends: 26/6 (14 h:56)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,68             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.077          ,50             ,5              ,146            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
11             ,34             ,True           ,True           ,73             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 68, True, 0.077, 50, 5, 146, 11, 34, True, True, 73
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (14h:56)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 146)      1101570     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        760         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 34)        256530      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 11)        1672        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 50, 151)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 170)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 55)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 73)           49275       concatenate_109[0][0]            
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 225)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 298)          0           phraseRnn[0][0]                  
                                                                 concatenate_110[0][0]            
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 68)           20332       concatenate_111[0][0]            
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 4)            276         dense_73[0][0]                   
==================================================================================================
Total params: 1,430,415
Trainable params: 1,430,415
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 280s - loss: 0.0699 - acc: 0.9821 - val_loss: 0.0552 - val_acc: 0.9857
Epoch 2/40
 - 279s - loss: 0.0498 - acc: 0.9877 - val_loss: 0.0560 - val_acc: 0.9857
Epoch 3/40
 - 279s - loss: 0.0468 - acc: 0.9887 - val_loss: 0.0573 - val_acc: 0.9859
Epoch 4/40
 - 279s - loss: 0.0452 - acc: 0.9892 - val_loss: 0.0611 - val_acc: 0.9859
Epoch 5/40
 - 279s - loss: 0.0442 - acc: 0.9894 - val_loss: 0.0639 - val_acc: 0.9857
Epoch 00005: early stopping
	TRAINING TIME: 24.88 minutes 
==================================================================================================
	PARSING TIME: 7.98 minutes 
==================================================================================================
	Identification : 0.576
	P, R  : 0.643, 0.522

==================================================================================================
	XP Ends: 26/6 (15 h:29)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (15h:29)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 146)      1057186     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        845         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 34)        246194      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 11)        1859        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 50, 151)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 170)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 55)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 73)           49275       concatenate_112[0][0]            
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 225)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 298)          0           phraseRnn[0][0]                  
                                                                 concatenate_113[0][0]            
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 68)           20332       concatenate_114[0][0]            
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 4)            276         dense_75[0][0]                   
==================================================================================================
Total params: 1,375,967
Trainable params: 1,375,967
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 189s - loss: 0.0797 - acc: 0.9791 - val_loss: 0.0597 - val_acc: 0.9848
Epoch 2/40
 - 189s - loss: 0.0526 - acc: 0.9869 - val_loss: 0.0604 - val_acc: 0.9849
Epoch 3/40
 - 189s - loss: 0.0485 - acc: 0.9881 - val_loss: 0.0624 - val_acc: 0.9848
Epoch 4/40
 - 202s - loss: 0.0470 - acc: 0.9886 - val_loss: 0.0655 - val_acc: 0.9849
Epoch 5/40
 - 189s - loss: 0.0463 - acc: 0.9887 - val_loss: 0.0692 - val_acc: 0.9849
Epoch 00005: early stopping
	TRAINING TIME: 16.92 minutes 
==================================================================================================
	PARSING TIME: 12.4 minutes 
==================================================================================================
	Identification : 0.495
	P, R  : 0.411, 0.622

==================================================================================================
	XP Ends: 26/6 (15 h:59)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (15h:59)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 146)      2013778     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        545         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 34)        468962      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 11)        1199        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 50, 151)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 170)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 55)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 73)           49275       concatenate_115[0][0]            
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 225)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 298)          0           phraseRnn[0][0]                  
                                                                 concatenate_116[0][0]            
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 68)           20332       concatenate_117[0][0]            
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 4)            276         dense_77[0][0]                   
==================================================================================================
Total params: 2,554,367
Trainable params: 2,554,367
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 395s - loss: 0.0717 - acc: 0.9811 - val_loss: 0.0581 - val_acc: 0.9843
Epoch 2/40
 - 377s - loss: 0.0520 - acc: 0.9867 - val_loss: 0.0595 - val_acc: 0.9843
Epoch 3/40
 - 377s - loss: 0.0490 - acc: 0.9878 - val_loss: 0.0611 - val_acc: 0.9841
Epoch 4/40
 - 378s - loss: 0.0471 - acc: 0.9884 - val_loss: 0.0669 - val_acc: 0.9839
Epoch 5/40
 - 377s - loss: 0.0460 - acc: 0.9888 - val_loss: 0.0715 - val_acc: 0.9836
Epoch 00005: early stopping
	TRAINING TIME: 34.6 minutes 
==================================================================================================
	PARSING TIME: 5.5 minutes 
==================================================================================================
	Identification : 0.497
	P, R  : 0.437, 0.575

==================================================================================================
	XP Ends: 26/6 (16 h:40)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,160            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.111          ,50             ,47             ,127            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
13             ,35             ,True           ,True           ,381            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 160, True, 0.111, 50, 47, 127, 13, 35, True, True, 381
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (16h:40)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 127)      1452626     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 47)       7144        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 35)        400330      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 13)        1976        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 50, 174)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 175)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 65)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 381)          635508      concatenate_118[0][0]            
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 240)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 621)          0           phraseRnn[0][0]                  
                                                                 concatenate_119[0][0]            
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 160)          99520       concatenate_120[0][0]            
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 4)            644         dense_79[0][0]                   
==================================================================================================
Total params: 2,597,748
Trainable params: 2,597,748
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 550s - loss: 12.0886 - acc: 0.2498 - val_loss: 12.0730 - val_acc: 0.2510
Epoch 2/40
 - 549s - loss: 12.0925 - acc: 0.2498 - val_loss: 12.0730 - val_acc: 0.2510
Epoch 3/40
 - 550s - loss: 12.0925 - acc: 0.2498 - val_loss: 12.0730 - val_acc: 0.2510
Epoch 4/40
 - 577s - loss: 12.0925 - acc: 0.2498 - val_loss: 12.0730 - val_acc: 0.2510
Epoch 5/40
 - 578s - loss: 12.0925 - acc: 0.2498 - val_loss: 12.0730 - val_acc: 0.2510
Epoch 00005: early stopping
	TRAINING TIME: 48.35 minutes 
==================================================================================================
	PARSING TIME: 16.82 minutes 
==================================================================================================
	Identification : 0
	P, R  : 0.0, 0.0

==================================================================================================
	XP Ends: 26/6 (17 h:45)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (17h:45)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 127)      1193927     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 47)       7943        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 35)        329035      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 13)        2197        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 50, 174)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 175)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 65)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 381)          635508      concatenate_121[0][0]            
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 240)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 621)          0           phraseRnn[0][0]                  
                                                                 concatenate_122[0][0]            
__________________________________________________________________________________________________
dense_81 (Dense)                (None, 160)          99520       concatenate_123[0][0]            
__________________________________________________________________________________________________
dense_82 (Dense)                (None, 4)            644         dense_81[0][0]                   
==================================================================================================
Total params: 2,268,774
Trainable params: 2,268,774
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 392s - loss: 12.1473 - acc: 0.2459 - val_loss: 12.0968 - val_acc: 0.2495
Epoch 2/40
 - 391s - loss: 12.1536 - acc: 0.2460 - val_loss: 12.0968 - val_acc: 0.2495
Epoch 3/40
 - 392s - loss: 12.1536 - acc: 0.2460 - val_loss: 12.0968 - val_acc: 0.2495
Epoch 4/40
 - 392s - loss: 12.1536 - acc: 0.2460 - val_loss: 12.0968 - val_acc: 0.2495
Epoch 5/40
 - 375s - loss: 12.1536 - acc: 0.2460 - val_loss: 12.0968 - val_acc: 0.2495
Epoch 00005: early stopping
	TRAINING TIME: 33.45 minutes 
==================================================================================================
	PARSING TIME: 26.2 minutes 
==================================================================================================
	Identification : 0
	P, R  : 0.0, 0.0

==================================================================================================
	XP Ends: 26/6 (18 h:46)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (18h:46)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 127)      2803017     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 47)       5123        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 35)        772485      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 13)        1417        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 50, 174)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 175)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 65)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 381)          635508      concatenate_124[0][0]            
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 240)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 621)          0           phraseRnn[0][0]                  
                                                                 concatenate_125[0][0]            
__________________________________________________________________________________________________
dense_83 (Dense)                (None, 160)          99520       concatenate_126[0][0]            
__________________________________________________________________________________________________
dense_84 (Dense)                (None, 4)            644         dense_83[0][0]                   
==================================================================================================
Total params: 4,317,714
Trainable params: 4,317,714
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 782s - loss: 10.3902 - acc: 0.3551 - val_loss: 8.0332 - val_acc: 0.5016
Epoch 2/40
 - 781s - loss: 8.0655 - acc: 0.4996 - val_loss: 8.0332 - val_acc: 0.5016
Epoch 3/40
 - 781s - loss: 8.0655 - acc: 0.4996 - val_loss: 8.0332 - val_acc: 0.5016
Epoch 4/40
 - 781s - loss: 8.0655 - acc: 0.4996 - val_loss: 8.0332 - val_acc: 0.5016
Epoch 5/40
 - 746s - loss: 8.0655 - acc: 0.4996 - val_loss: 8.0332 - val_acc: 0.5016
Epoch 00005: early stopping
	TRAINING TIME: 67.47 minutes 
==================================================================================================
	PARSING TIME: 10.03 minutes 
==================================================================================================
	Identification : 0
	P, R  : 0.0, 0.0

==================================================================================================
	XP Ends: 26/6 (20 h:4)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,334            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.075          ,50             ,24             ,29             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
22             ,115            ,True           ,True           ,42             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 334, True, 0.075, 50, 24, 29, 22, 115, True, True, 42
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (20h:4)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 29)       218805      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 24)       3648        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 115)       867675      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 22)        3344        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 50, 53)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 575)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 110)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 42)           12096       concatenate_127[0][0]            
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 685)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 727)          0           phraseRnn[0][0]                  
                                                                 concatenate_128[0][0]            
__________________________________________________________________________________________________
dense_85 (Dense)                (None, 334)          243152      concatenate_129[0][0]            
__________________________________________________________________________________________________
dense_86 (Dense)                (None, 4)            1340        dense_85[0][0]                   
==================================================================================================
Total params: 1,350,060
Trainable params: 1,350,060
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 269s - loss: 0.0849 - acc: 0.9812 - val_loss: 0.0566 - val_acc: 0.9853
Epoch 2/40
 - 269s - loss: 0.0503 - acc: 0.9875 - val_loss: 0.0583 - val_acc: 0.9852
Epoch 3/40
 - 270s - loss: 0.0472 - acc: 0.9886 - val_loss: 0.0595 - val_acc: 0.9859
Epoch 4/40
 - 268s - loss: 0.0456 - acc: 0.9890 - val_loss: 0.0638 - val_acc: 0.9856
Epoch 5/40
 - 270s - loss: 0.0447 - acc: 0.9893 - val_loss: 0.0668 - val_acc: 0.9857
Epoch 00005: early stopping
	TRAINING TIME: 23.9 minutes 
==================================================================================================
	PARSING TIME: 8.08 minutes 
==================================================================================================
	Identification : 0.548
	P, R  : 0.543, 0.554

==================================================================================================
	XP Ends: 26/6 (20 h:36)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (20h:36)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 29)       209989      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 24)       4056        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 115)       832715      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 22)        3718        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 50, 53)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 575)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 110)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 42)           12096       concatenate_130[0][0]            
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 685)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 727)          0           phraseRnn[0][0]                  
                                                                 concatenate_131[0][0]            
__________________________________________________________________________________________________
dense_87 (Dense)                (None, 334)          243152      concatenate_132[0][0]            
__________________________________________________________________________________________________
dense_88 (Dense)                (None, 4)            1340        dense_87[0][0]                   
==================================================================================================
Total params: 1,307,066
Trainable params: 1,307,066
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 184s - loss: 0.0907 - acc: 0.9791 - val_loss: 0.0615 - val_acc: 0.9844
Epoch 2/40
 - 182s - loss: 0.0520 - acc: 0.9871 - val_loss: 0.0616 - val_acc: 0.9845
Epoch 3/40
 - 181s - loss: 0.0485 - acc: 0.9882 - val_loss: 0.0652 - val_acc: 0.9847
Epoch 4/40
 - 182s - loss: 0.0468 - acc: 0.9885 - val_loss: 0.0700 - val_acc: 0.9847
Epoch 5/40
 - 182s - loss: 0.0459 - acc: 0.9888 - val_loss: 0.0834 - val_acc: 0.9847
Epoch 00005: early stopping
	TRAINING TIME: 16.2 minutes 
==================================================================================================
	PARSING TIME: 13.9 minutes 
==================================================================================================
	Identification : 0.49
	P, R  : 0.39, 0.66

==================================================================================================
	XP Ends: 26/6 (21 h:7)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (21h:7)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 29)       399997      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 24)       2616        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 115)       1586195     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 22)        2398        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 50, 53)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 575)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 110)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 42)           12096       concatenate_133[0][0]            
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 685)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 727)          0           phraseRnn[0][0]                  
                                                                 concatenate_134[0][0]            
__________________________________________________________________________________________________
dense_89 (Dense)                (None, 334)          243152      concatenate_135[0][0]            
__________________________________________________________________________________________________
dense_90 (Dense)                (None, 4)            1340        dense_89[0][0]                   
==================================================================================================
Total params: 2,247,794
Trainable params: 2,247,794
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 361s - loss: 0.0742 - acc: 0.9813 - val_loss: 0.0578 - val_acc: 0.9844
Epoch 2/40
 - 363s - loss: 0.0518 - acc: 0.9868 - val_loss: 0.0578 - val_acc: 0.9848
Epoch 3/40
 - 397s - loss: 0.0482 - acc: 0.9880 - val_loss: 0.0624 - val_acc: 0.9843
Epoch 4/40
 - 360s - loss: 0.0463 - acc: 0.9886 - val_loss: 0.0719 - val_acc: 0.9841
Epoch 5/40
 - 359s - loss: 0.0451 - acc: 0.9889 - val_loss: 0.0798 - val_acc: 0.9841
Epoch 00005: early stopping
	TRAINING TIME: 33.68 minutes 
==================================================================================================
	PARSING TIME: 6.13 minutes 
==================================================================================================
	Identification : 0.493
	P, R  : 0.447, 0.549

==================================================================================================
	XP Ends: 26/6 (21 h:47)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,53             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.176          ,50             ,15             ,185            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
10             ,134            ,True           ,True           ,142            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 53, True, 0.176, 50, 15, 185, 10, 134, True, True, 142
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (21h:47)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 185)      1395825     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 15)       2280        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 134)       1011030     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 10)        1520        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 50, 200)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 670)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 50)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 142)          146118      concatenate_136[0][0]            
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 720)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 862)          0           phraseRnn[0][0]                  
                                                                 concatenate_137[0][0]            
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 53)           45739       concatenate_138[0][0]            
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 4)            216         dense_91[0][0]                   
==================================================================================================
Total params: 2,602,728
Trainable params: 2,602,728
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 342s - loss: 0.5493 - acc: 0.9540 - val_loss: 0.0670 - val_acc: 0.9840
Epoch 2/40
 - 341s - loss: 0.0568 - acc: 0.9857 - val_loss: 0.0609 - val_acc: 0.9844
Epoch 3/40
 - 341s - loss: 0.0500 - acc: 0.9875 - val_loss: 0.0593 - val_acc: 0.9850
Epoch 4/40
 - 341s - loss: 0.0476 - acc: 0.9883 - val_loss: 0.0619 - val_acc: 0.9851
Epoch 5/40
 - 341s - loss: 0.0464 - acc: 0.9887 - val_loss: 0.0650 - val_acc: 0.9850
Epoch 00005: early stopping
	TRAINING TIME: 30.02 minutes 
==================================================================================================
	PARSING TIME: 8.17 minutes 
==================================================================================================
	Identification : 0.582
	P, R  : 0.624, 0.546

==================================================================================================
	XP Ends: 26/6 (22 h:26)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 26/6 (22h:26)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 185)      1339585     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 15)       2535        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 134)       970294      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 10)        1690        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 50, 200)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 670)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 50)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 142)          146118      concatenate_139[0][0]            
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 720)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 862)          0           phraseRnn[0][0]                  
                                                                 concatenate_140[0][0]            
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 53)           45739       concatenate_141[0][0]            
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 4)            216         dense_93[0][0]                   
==================================================================================================
Total params: 2,506,177
Trainable params: 2,506,177
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 240s - loss: 4.4807 - acc: 0.7215 - val_loss: 4.5237 - val_acc: 0.7193
Epoch 2/40
 - 243s - loss: 4.4618 - acc: 0.7232 - val_loss: 4.5237 - val_acc: 0.7193
Epoch 3/40
 - 244s - loss: 4.4618 - acc: 0.7232 - val_loss: 4.5237 - val_acc: 0.7193
Epoch 4/40
 - 232s - loss: 4.4618 - acc: 0.7232 - val_loss: 4.5237 - val_acc: 0.7193
Epoch 5/40
 - 232s - loss: 4.4618 - acc: 0.7232 - val_loss: 4.5237 - val_acc: 0.7193
Epoch 00005: early stopping
	TRAINING TIME: 20.87 minutes 
==================================================================================================
	PARSING TIME: 12.22 minutes 
==================================================================================================
	Identification : 0
	P, R  : 0.0, 0.0

==================================================================================================
	XP Ends: 26/6 (22 h:59)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (22h:59)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 185)      2551705     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 15)       1635        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 134)       1848262     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 10)        1090        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 50, 200)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 670)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 50)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 142)          146118      concatenate_142[0][0]            
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 720)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 862)          0           phraseRnn[0][0]                  
                                                                 concatenate_143[0][0]            
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 53)           45739       concatenate_144[0][0]            
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 4)            216         dense_95[0][0]                   
==================================================================================================
Total params: 4,594,765
Trainable params: 4,594,765
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 462s - loss: 2.3223 - acc: 0.8460 - val_loss: 0.0688 - val_acc: 0.9818
Epoch 2/40
 - 488s - loss: 0.0608 - acc: 0.9843 - val_loss: 0.0631 - val_acc: 0.9831
Epoch 3/40
 - 487s - loss: 0.0521 - acc: 0.9868 - val_loss: 0.0620 - val_acc: 0.9838
Epoch 4/40
 - 486s - loss: 0.0493 - acc: 0.9878 - val_loss: 0.0648 - val_acc: 0.9836
Epoch 5/40
 - 488s - loss: 0.0478 - acc: 0.9882 - val_loss: 0.0698 - val_acc: 0.9837
Epoch 00005: early stopping
	TRAINING TIME: 43.12 minutes 
==================================================================================================
	PARSING TIME: 6.15 minutes 
==================================================================================================
	Identification : 0.47
	P, R  : 0.596, 0.388

==================================================================================================
	XP Ends: 26/6 (23 h:49)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,50             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.106          ,50             ,12             ,89             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
8              ,164            ,True           ,True           ,276            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 50, True, 0.106, 50, 12, 89, 8, 164, True, True, 276
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 26/6 (23h:49)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 89)       671505      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       1824        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 164)       1237380     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 8)         1216        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 50, 101)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 820)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 40)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 276)          312984      concatenate_145[0][0]            
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 860)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 1136)         0           phraseRnn[0][0]                  
                                                                 concatenate_146[0][0]            
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 50)           56850       concatenate_147[0][0]            
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 4)            204         dense_97[0][0]                   
==================================================================================================
Total params: 2,281,963
Trainable params: 2,281,963
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 421s - loss: 0.1738 - acc: 0.9739 - val_loss: 0.0586 - val_acc: 0.9846
Epoch 2/40
 - 422s - loss: 0.0523 - acc: 0.9866 - val_loss: 0.0595 - val_acc: 0.9852
Epoch 3/40
 - 421s - loss: 0.0486 - acc: 0.9880 - val_loss: 0.0597 - val_acc: 0.9855
Epoch 4/40
 - 422s - loss: 0.0468 - acc: 0.9886 - val_loss: 0.0632 - val_acc: 0.9854
Epoch 5/40
 - 421s - loss: 0.0458 - acc: 0.9889 - val_loss: 0.0645 - val_acc: 0.9853
Epoch 00005: early stopping
	TRAINING TIME: 36.7 minutes 
==================================================================================================
	PARSING TIME: 8.17 minutes 
==================================================================================================
	Identification : 0.598
	P, R  : 0.765, 0.491

==================================================================================================
	XP Ends: 27/6 (0 h:34)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (0h:34)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 89)       644449      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       2028        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 164)       1187524     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 8)         1352        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 50, 101)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 820)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 40)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 276)          312984      concatenate_148[0][0]            
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 860)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 1136)         0           phraseRnn[0][0]                  
                                                                 concatenate_149[0][0]            
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 50)           56850       concatenate_150[0][0]            
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 4)            204         dense_99[0][0]                   
==================================================================================================
Total params: 2,205,391
Trainable params: 2,205,391
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 285s - loss: 0.8112 - acc: 0.9321 - val_loss: 0.0657 - val_acc: 0.9830
Epoch 2/40
 - 285s - loss: 0.0567 - acc: 0.9856 - val_loss: 0.0630 - val_acc: 0.9836
Epoch 3/40
 - 285s - loss: 0.0514 - acc: 0.9871 - val_loss: 0.0652 - val_acc: 0.9840
Epoch 4/40
 - 285s - loss: 0.0492 - acc: 0.9878 - val_loss: 0.0670 - val_acc: 0.9840
Epoch 5/40
 - 285s - loss: 0.0479 - acc: 0.9881 - val_loss: 0.0688 - val_acc: 0.9839
Epoch 00005: early stopping
	TRAINING TIME: 24.77 minutes 
==================================================================================================
	PARSING TIME: 12.47 minutes 
==================================================================================================
	Identification : 0.508
	P, R  : 0.44, 0.6

==================================================================================================
	XP Ends: 27/6 (1 h:12)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (1h:12)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 89)       1227577     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       1308        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 164)       2262052     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 8)         872         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 50, 101)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 820)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 40)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 276)          312984      concatenate_151[0][0]            
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 860)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 1136)         0           phraseRnn[0][0]                  
                                                                 concatenate_152[0][0]            
__________________________________________________________________________________________________
dense_101 (Dense)               (None, 50)           56850       concatenate_153[0][0]            
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 4)            204         dense_101[0][0]                  
==================================================================================================
Total params: 3,861,847
Trainable params: 3,861,847
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 597s - loss: 0.1940 - acc: 0.9733 - val_loss: 0.0592 - val_acc: 0.9841
Epoch 2/40
 - 597s - loss: 0.0531 - acc: 0.9863 - val_loss: 0.0603 - val_acc: 0.9840
Epoch 3/40
 - 597s - loss: 0.0499 - acc: 0.9875 - val_loss: 0.0622 - val_acc: 0.9838
Epoch 4/40
 - 597s - loss: 0.0481 - acc: 0.9881 - val_loss: 0.0719 - val_acc: 0.9833
Epoch 5/40
 - 596s - loss: 0.0470 - acc: 0.9884 - val_loss: 0.0754 - val_acc: 0.9838
Epoch 00005: early stopping
	TRAINING TIME: 52.7 minutes 
==================================================================================================
	PARSING TIME: 6.28 minutes 
==================================================================================================
	Identification : 0.49
	P, R  : 0.497, 0.484

==================================================================================================
	XP Ends: 27/6 (2 h:12)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,422            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.021          ,50             ,21             ,35             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
27             ,191            ,True           ,True           ,86             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 422, True, 0.021, 50, 21, 35, 27, 191, True, True, 86
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (2h:12)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 35)       264075      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 21)       3192        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 191)       1441095     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 27)        4104        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 50, 56)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 955)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 135)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 86)           36894       concatenate_154[0][0]            
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 1090)         0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 1176)         0           phraseRnn[0][0]                  
                                                                 concatenate_155[0][0]            
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 422)          496694      concatenate_156[0][0]            
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 4)            1692        dense_103[0][0]                  
==================================================================================================
Total params: 2,247,746
Trainable params: 2,247,746
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 277s - loss: 0.0640 - acc: 0.9831 - val_loss: 0.0544 - val_acc: 0.9859
Epoch 2/40
 - 276s - loss: 0.0494 - acc: 0.9878 - val_loss: 0.0547 - val_acc: 0.9862
Epoch 3/40
 - 302s - loss: 0.0466 - acc: 0.9887 - val_loss: 0.0584 - val_acc: 0.9859
Epoch 4/40
 - 301s - loss: 0.0451 - acc: 0.9892 - val_loss: 0.0641 - val_acc: 0.9859
Epoch 5/40
 - 302s - loss: 0.0442 - acc: 0.9894 - val_loss: 0.0702 - val_acc: 0.9857
Epoch 00005: early stopping
	TRAINING TIME: 25.98 minutes 
==================================================================================================
	PARSING TIME: 7.95 minutes 
==================================================================================================
	Identification : 0.574
	P, R  : 0.632, 0.525

==================================================================================================
	XP Ends: 27/6 (2 h:46)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (2h:46)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 35)       253435      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 21)       3549        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 191)       1383031     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 27)        4563        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 50, 56)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 955)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 135)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 86)           36894       concatenate_157[0][0]            
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 1090)         0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 1176)         0           phraseRnn[0][0]                  
                                                                 concatenate_158[0][0]            
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 422)          496694      concatenate_159[0][0]            
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 4)            1692        dense_105[0][0]                  
==================================================================================================
Total params: 2,179,858
Trainable params: 2,179,858
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 183s - loss: 0.0718 - acc: 0.9802 - val_loss: 0.0621 - val_acc: 0.9840
Epoch 2/40
 - 183s - loss: 0.0519 - acc: 0.9869 - val_loss: 0.0596 - val_acc: 0.9846
Epoch 3/40
 - 183s - loss: 0.0483 - acc: 0.9882 - val_loss: 0.0628 - val_acc: 0.9844
Epoch 4/40
 - 182s - loss: 0.0469 - acc: 0.9886 - val_loss: 0.0667 - val_acc: 0.9845
Epoch 5/40
 - 182s - loss: 0.0460 - acc: 0.9887 - val_loss: 0.0733 - val_acc: 0.9844
Epoch 00005: early stopping
	TRAINING TIME: 16.23 minutes 
==================================================================================================
	PARSING TIME: 12.35 minutes 
==================================================================================================
	Identification : 0.531
	P, R  : 0.476, 0.6

==================================================================================================
	XP Ends: 27/6 (3 h:15)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (3h:15)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 35)       482755      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 21)       2289        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 191)       2634463     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 27)        2943        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 50, 56)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 955)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 135)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 86)           36894       concatenate_160[0][0]            
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 1090)         0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 1176)         0           phraseRnn[0][0]                  
                                                                 concatenate_161[0][0]            
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 422)          496694      concatenate_162[0][0]            
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 4)            1692        dense_107[0][0]                  
==================================================================================================
Total params: 3,657,730
Trainable params: 3,657,730
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 364s - loss: 0.0643 - acc: 0.9824 - val_loss: 0.0559 - val_acc: 0.9849
Epoch 2/40
 - 365s - loss: 0.0510 - acc: 0.9870 - val_loss: 0.0571 - val_acc: 0.9845
Epoch 3/40
 - 365s - loss: 0.0480 - acc: 0.9881 - val_loss: 0.0608 - val_acc: 0.9843
Epoch 4/40
 - 364s - loss: 0.0462 - acc: 0.9887 - val_loss: 0.0695 - val_acc: 0.9843
Epoch 5/40
 - 363s - loss: 0.0450 - acc: 0.9890 - val_loss: 0.0764 - val_acc: 0.9840
Epoch 00005: early stopping
	TRAINING TIME: 33.33 minutes 
==================================================================================================
	PARSING TIME: 5.67 minutes 
==================================================================================================
	Identification : 0.513
	P, R  : 0.465, 0.573

==================================================================================================
	XP Ends: 27/6 (3 h:55)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,191            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.09           ,50             ,5              ,124            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
27             ,176            ,True           ,True           ,30             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 191, True, 0.09, 50, 5, 124, 27, 176, True, True, 30
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (3h:55)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 124)      1418312     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        760         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 176)       2013088     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 27)        4104        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 50, 129)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 880)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 135)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 30)           14400       concatenate_163[0][0]            
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 1015)         0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 1045)         0           phraseRnn[0][0]                  
                                                                 concatenate_164[0][0]            
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 191)          199786      concatenate_165[0][0]            
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 4)            768         dense_109[0][0]                  
==================================================================================================
Total params: 3,651,218
Trainable params: 3,651,218
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 275s - loss: 0.0792 - acc: 0.9818 - val_loss: 0.0543 - val_acc: 0.9861
Epoch 2/40
 - 304s - loss: 0.0493 - acc: 0.9880 - val_loss: 0.0551 - val_acc: 0.9863
Epoch 3/40
 - 302s - loss: 0.0467 - acc: 0.9887 - val_loss: 0.0568 - val_acc: 0.9866
Epoch 4/40
 - 300s - loss: 0.0453 - acc: 0.9891 - val_loss: 0.0603 - val_acc: 0.9863
Epoch 5/40
 - 273s - loss: 0.0445 - acc: 0.9892 - val_loss: 0.0670 - val_acc: 0.9862
Epoch 00005: early stopping
	TRAINING TIME: 25.83 minutes 
==================================================================================================
	PARSING TIME: 7.95 minutes 
==================================================================================================
	Identification : 0.544
	P, R  : 0.528, 0.561

==================================================================================================
	XP Ends: 27/6 (4 h:29)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (4h:29)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 124)      1165724     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        845         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 176)       1654576     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 27)        4563        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_166 (Concatenate)   (None, 50, 129)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 880)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 135)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 30)           14400       concatenate_166[0][0]            
__________________________________________________________________________________________________
concatenate_167 (Concatenate)   (None, 1015)         0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_168 (Concatenate)   (None, 1045)         0           phraseRnn[0][0]                  
                                                                 concatenate_167[0][0]            
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 191)          199786      concatenate_168[0][0]            
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 4)            768         dense_111[0][0]                  
==================================================================================================
Total params: 3,040,662
Trainable params: 3,040,662
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 205s - loss: 0.0996 - acc: 0.9777 - val_loss: 0.0590 - val_acc: 0.9847
Epoch 2/40
 - 204s - loss: 0.0524 - acc: 0.9870 - val_loss: 0.0593 - val_acc: 0.9851
Epoch 3/40
 - 204s - loss: 0.0487 - acc: 0.9881 - val_loss: 0.0622 - val_acc: 0.9853
Epoch 4/40
 - 204s - loss: 0.0472 - acc: 0.9884 - val_loss: 0.0680 - val_acc: 0.9850
Epoch 5/40
 - 204s - loss: 0.0462 - acc: 0.9886 - val_loss: 0.0766 - val_acc: 0.9850
Epoch 00005: early stopping
	TRAINING TIME: 18.03 minutes 
==================================================================================================
	PARSING TIME: 12.25 minutes 
==================================================================================================
	Identification : 0.46
	P, R  : 0.378, 0.588

==================================================================================================
	XP Ends: 27/6 (5 h:0)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (5h:0)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 124)      2736804     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 5)        545         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 176)       3884496     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 27)        2943        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_169 (Concatenate)   (None, 50, 129)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 880)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 135)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 30)           14400       concatenate_169[0][0]            
__________________________________________________________________________________________________
concatenate_170 (Concatenate)   (None, 1015)         0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_171 (Concatenate)   (None, 1045)         0           phraseRnn[0][0]                  
                                                                 concatenate_170[0][0]            
__________________________________________________________________________________________________
dense_113 (Dense)               (None, 191)          199786      concatenate_171[0][0]            
__________________________________________________________________________________________________
dense_114 (Dense)               (None, 4)            768         dense_113[0][0]                  
==================================================================================================
Total params: 6,839,742
Trainable params: 6,839,742
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 376s - loss: 0.1131 - acc: 0.9797 - val_loss: 0.0610 - val_acc: 0.9834
Epoch 2/40
 - 370s - loss: 0.0488 - acc: 0.9879 - val_loss: 0.0605 - val_acc: 0.9843
Epoch 3/40
 - 370s - loss: 0.0459 - acc: 0.9889 - val_loss: 0.0650 - val_acc: 0.9841
Epoch 4/40
 - 373s - loss: 0.0446 - acc: 0.9893 - val_loss: 0.0758 - val_acc: 0.9840
Epoch 5/40
 - 370s - loss: 0.0433 - acc: 0.9894 - val_loss: 0.0876 - val_acc: 0.9838
Epoch 00005: early stopping
	TRAINING TIME: 33.93 minutes 
==================================================================================================
	PARSING TIME: 5.53 minutes 
==================================================================================================
	Identification : 0.457
	P, R  : 0.399, 0.535

==================================================================================================
	XP Ends: 27/6 (5 h:40)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,277            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.072          ,50             ,12             ,75             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
16             ,138            ,True           ,False          ,39             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 277, True, 0.072, 50, 12, 75, 16, 138, True, False, 39
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (5h:40)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 75)       565875      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       1824        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 138)       1041210     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 16)        2432        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_172 (Concatenate)   (None, 50, 87)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 552)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 64)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 39)           14859       concatenate_172[0][0]            
__________________________________________________________________________________________________
concatenate_173 (Concatenate)   (None, 616)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_174 (Concatenate)   (None, 655)          0           phraseRnn[0][0]                  
                                                                 concatenate_173[0][0]            
__________________________________________________________________________________________________
dense_115 (Dense)               (None, 277)          181712      concatenate_174[0][0]            
__________________________________________________________________________________________________
dense_116 (Dense)               (None, 4)            1112        dense_115[0][0]                  
==================================================================================================
Total params: 1,809,024
Trainable params: 1,809,024
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 271s - loss: 0.0727 - acc: 0.9822 - val_loss: 0.0568 - val_acc: 0.9855
Epoch 2/40
 - 269s - loss: 0.0504 - acc: 0.9874 - val_loss: 0.0573 - val_acc: 0.9855
Epoch 3/40
 - 271s - loss: 0.0475 - acc: 0.9884 - val_loss: 0.0598 - val_acc: 0.9856
Epoch 4/40
 - 272s - loss: 0.0458 - acc: 0.9889 - val_loss: 0.0644 - val_acc: 0.9855
Epoch 5/40
 - 271s - loss: 0.0448 - acc: 0.9892 - val_loss: 0.0706 - val_acc: 0.9851
Epoch 00005: early stopping
	TRAINING TIME: 24.22 minutes 
==================================================================================================
	PARSING TIME: 8.13 minutes 
==================================================================================================
	Identification : 0.565
	P, R  : 0.595, 0.537

==================================================================================================
	XP Ends: 27/6 (6 h:13)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (6h:13)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 75)       543075      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       2028        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 138)       999258      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 16)        2704        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_175 (Concatenate)   (None, 50, 87)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 552)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 64)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 39)           14859       concatenate_175[0][0]            
__________________________________________________________________________________________________
concatenate_176 (Concatenate)   (None, 616)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_177 (Concatenate)   (None, 655)          0           phraseRnn[0][0]                  
                                                                 concatenate_176[0][0]            
__________________________________________________________________________________________________
dense_117 (Dense)               (None, 277)          181712      concatenate_177[0][0]            
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 4)            1112        dense_117[0][0]                  
==================================================================================================
Total params: 1,744,748
Trainable params: 1,744,748
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 182s - loss: 0.0846 - acc: 0.9788 - val_loss: 0.0618 - val_acc: 0.9833
Epoch 2/40
 - 182s - loss: 0.0548 - acc: 0.9861 - val_loss: 0.0647 - val_acc: 0.9837
Epoch 3/40
 - 182s - loss: 0.0508 - acc: 0.9872 - val_loss: 0.0665 - val_acc: 0.9838
Epoch 4/40
 - 181s - loss: 0.0483 - acc: 0.9881 - val_loss: 0.0709 - val_acc: 0.9840
Epoch 5/40
 - 182s - loss: 0.0469 - acc: 0.9884 - val_loss: 0.0792 - val_acc: 0.9838
Epoch 00005: early stopping
	TRAINING TIME: 16.15 minutes 
==================================================================================================
	PARSING TIME: 12.58 minutes 
==================================================================================================
	Identification : 0.487
	P, R  : 0.415, 0.588

==================================================================================================
	XP Ends: 27/6 (6 h:42)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (6h:42)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 75)       1034475     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       1308        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 138)       1903434     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 16)        1744        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_178 (Concatenate)   (None, 50, 87)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 552)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 64)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 39)           14859       concatenate_178[0][0]            
__________________________________________________________________________________________________
concatenate_179 (Concatenate)   (None, 616)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_180 (Concatenate)   (None, 655)          0           phraseRnn[0][0]                  
                                                                 concatenate_179[0][0]            
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 277)          181712      concatenate_180[0][0]            
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 4)            1112        dense_119[0][0]                  
==================================================================================================
Total params: 3,138,644
Trainable params: 3,138,644
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 368s - loss: 3.0292 - acc: 0.8021 - val_loss: 0.0659 - val_acc: 0.9832
Epoch 2/40
 - 366s - loss: 0.0563 - acc: 0.9853 - val_loss: 0.0587 - val_acc: 0.9840
Epoch 3/40
 - 363s - loss: 0.0505 - acc: 0.9873 - val_loss: 0.0622 - val_acc: 0.9838
Epoch 4/40
 - 366s - loss: 0.0477 - acc: 0.9881 - val_loss: 0.0702 - val_acc: 0.9836
Epoch 5/40
 - 363s - loss: 0.0461 - acc: 0.9886 - val_loss: 0.0777 - val_acc: 0.9835
Epoch 00005: early stopping
	TRAINING TIME: 33.43 minutes 
==================================================================================================
	PARSING TIME: 5.43 minutes 
==================================================================================================
	Identification : 0.492
	P, R  : 0.486, 0.498

==================================================================================================
	XP Ends: 27/6 (7 h:21)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,59             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.038          ,50             ,26             ,29             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
7              ,45             ,True           ,False          ,53             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 59, True, 0.038, 50, 26, 29, 7, 45, True, False, 53
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (7h:21)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 29)       218805      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 26)       3952        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 45)        339525      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 7)         1064        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_181 (Concatenate)   (None, 50, 55)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 180)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 28)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 53)           17331       concatenate_181[0][0]            
__________________________________________________________________________________________________
concatenate_182 (Concatenate)   (None, 208)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_183 (Concatenate)   (None, 261)          0           phraseRnn[0][0]                  
                                                                 concatenate_182[0][0]            
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 59)           15458       concatenate_183[0][0]            
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 4)            240         dense_121[0][0]                  
==================================================================================================
Total params: 596,375
Trainable params: 596,375
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 268s - loss: 0.0656 - acc: 0.9827 - val_loss: 0.0568 - val_acc: 0.9853
Epoch 2/40
 - 280s - loss: 0.0516 - acc: 0.9871 - val_loss: 0.0579 - val_acc: 0.9853
Epoch 3/40
 - 268s - loss: 0.0484 - acc: 0.9881 - val_loss: 0.0590 - val_acc: 0.9854
Epoch 4/40
 - 268s - loss: 0.0466 - acc: 0.9888 - val_loss: 0.0617 - val_acc: 0.9852
Epoch 5/40
 - 269s - loss: 0.0454 - acc: 0.9890 - val_loss: 0.0642 - val_acc: 0.9852
Epoch 00005: early stopping
	TRAINING TIME: 24.08 minutes 
==================================================================================================
	PARSING TIME: 8.07 minutes 
==================================================================================================
	Identification : 0.579
	P, R  : 0.713, 0.487

==================================================================================================
	XP Ends: 27/6 (7 h:54)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (7h:54)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 29)       209989      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 26)       4394        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 45)        325845      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 7)         1183        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_184 (Concatenate)   (None, 50, 55)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 180)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 28)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 53)           17331       concatenate_184[0][0]            
__________________________________________________________________________________________________
concatenate_185 (Concatenate)   (None, 208)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_186 (Concatenate)   (None, 261)          0           phraseRnn[0][0]                  
                                                                 concatenate_185[0][0]            
__________________________________________________________________________________________________
dense_123 (Dense)               (None, 59)           15458       concatenate_186[0][0]            
__________________________________________________________________________________________________
dense_124 (Dense)               (None, 4)            240         dense_123[0][0]                  
==================================================================================================
Total params: 574,440
Trainable params: 574,440
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 181s - loss: 0.0746 - acc: 0.9796 - val_loss: 0.0622 - val_acc: 0.9836
Epoch 2/40
 - 181s - loss: 0.0557 - acc: 0.9856 - val_loss: 0.0617 - val_acc: 0.9840
Epoch 3/40
 - 181s - loss: 0.0513 - acc: 0.9871 - val_loss: 0.0643 - val_acc: 0.9838
Epoch 4/40
 - 180s - loss: 0.0492 - acc: 0.9879 - val_loss: 0.0677 - val_acc: 0.9833
Epoch 5/40
 - 180s - loss: 0.0479 - acc: 0.9881 - val_loss: 0.0714 - val_acc: 0.9834
Epoch 00005: early stopping
	TRAINING TIME: 16.03 minutes 
==================================================================================================
	PARSING TIME: 14.02 minutes 
==================================================================================================
	Identification : 0.456
	P, R  : 0.38, 0.571

==================================================================================================
	XP Ends: 27/6 (8 h:24)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (8h:24)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 29)       399997      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 26)       2834        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 45)        620685      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 7)         763         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_187 (Concatenate)   (None, 50, 55)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 180)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 28)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 53)           17331       concatenate_187[0][0]            
__________________________________________________________________________________________________
concatenate_188 (Concatenate)   (None, 208)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_189 (Concatenate)   (None, 261)          0           phraseRnn[0][0]                  
                                                                 concatenate_188[0][0]            
__________________________________________________________________________________________________
dense_125 (Dense)               (None, 59)           15458       concatenate_189[0][0]            
__________________________________________________________________________________________________
dense_126 (Dense)               (None, 4)            240         dense_125[0][0]                  
==================================================================================================
Total params: 1,057,308
Trainable params: 1,057,308
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 363s - loss: 0.0660 - acc: 0.9820 - val_loss: 0.0573 - val_acc: 0.9842
Epoch 2/40
 - 365s - loss: 0.0529 - acc: 0.9863 - val_loss: 0.0580 - val_acc: 0.9839
Epoch 3/40
 - 367s - loss: 0.0498 - acc: 0.9875 - val_loss: 0.0626 - val_acc: 0.9836
Epoch 4/40
 - 363s - loss: 0.0480 - acc: 0.9881 - val_loss: 0.0645 - val_acc: 0.9835
Epoch 5/40
 - 361s - loss: 0.0467 - acc: 0.9885 - val_loss: 0.0697 - val_acc: 0.9833
Epoch 00005: early stopping
	TRAINING TIME: 33.23 minutes 
==================================================================================================
	PARSING TIME: 5.4 minutes 
==================================================================================================
	Identification : 0.474
	P, R  : 0.513, 0.441

==================================================================================================
	XP Ends: 27/6 (9 h:4)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,111            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.105          ,50             ,12             ,58             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
35             ,87             ,False          ,True           ,73             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 111, True, 0.105, 50, 12, 58, 35, 87, False, True, 73
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (9h:4)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 58)       663404      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       1824        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 87)        995106      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 35)        5320        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_190 (Concatenate)   (None, 50, 70)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 348)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 140)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 73)           31536       concatenate_190[0][0]            
__________________________________________________________________________________________________
concatenate_191 (Concatenate)   (None, 488)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_192 (Concatenate)   (None, 561)          0           phraseRnn[0][0]                  
                                                                 concatenate_191[0][0]            
__________________________________________________________________________________________________
dense_127 (Dense)               (None, 111)          62382       concatenate_192[0][0]            
__________________________________________________________________________________________________
dense_128 (Dense)               (None, 4)            448         dense_127[0][0]                  
==================================================================================================
Total params: 1,760,020
Trainable params: 1,760,020
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 272s - loss: 12.0770 - acc: 0.2503 - val_loss: 12.0999 - val_acc: 0.2493
Epoch 2/40
 - 273s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.0999 - val_acc: 0.2493
Epoch 3/40
 - 271s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.0999 - val_acc: 0.2493
Epoch 4/40
 - 271s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.0999 - val_acc: 0.2493
Epoch 5/40
 - 301s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.0999 - val_acc: 0.2493
Epoch 00005: early stopping
	TRAINING TIME: 24.67 minutes 
==================================================================================================
	PARSING TIME: 7.63 minutes 
==================================================================================================
	Identification : 0
	P, R  : 0.0, 0.0

==================================================================================================
	XP Ends: 27/6 (9 h:36)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (9h:36)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 58)       545258      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       2028        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 87)        817887      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 35)        5915        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_193 (Concatenate)   (None, 50, 70)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 348)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 140)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 73)           31536       concatenate_193[0][0]            
__________________________________________________________________________________________________
concatenate_194 (Concatenate)   (None, 488)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_195 (Concatenate)   (None, 561)          0           phraseRnn[0][0]                  
                                                                 concatenate_194[0][0]            
__________________________________________________________________________________________________
dense_129 (Dense)               (None, 111)          62382       concatenate_195[0][0]            
__________________________________________________________________________________________________
dense_130 (Dense)               (None, 4)            448         dense_129[0][0]                  
==================================================================================================
Total params: 1,465,454
Trainable params: 1,465,454
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 186s - loss: 0.1053 - acc: 0.9772 - val_loss: 0.0603 - val_acc: 0.9843
Epoch 2/40
 - 185s - loss: 0.0537 - acc: 0.9865 - val_loss: 0.0618 - val_acc: 0.9844
Epoch 3/40
 - 183s - loss: 0.0499 - acc: 0.9876 - val_loss: 0.2017 - val_acc: 0.9698
Epoch 4/40
 - 185s - loss: 0.0484 - acc: 0.9881 - val_loss: 0.0750 - val_acc: 0.9847
Epoch 5/40
 - 183s - loss: 0.0469 - acc: 0.9885 - val_loss: 0.0770 - val_acc: 0.9845
Epoch 00005: early stopping
	TRAINING TIME: 16.45 minutes 
==================================================================================================
	PARSING TIME: 12.17 minutes 
==================================================================================================
	Identification : 0.432
	P, R  : 0.329, 0.627

==================================================================================================
	XP Ends: 27/6 (10 h:5)
==================================================================================================
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '5039')
INFO:theano.gof.compilelock:Waiting for existing lock by unknown process (I am process '5039')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
INFO:theano.gof.compilelock:To manually release the lock, delete /home/halsaied/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.9--2.7.14-64/lock_dir
# Seed: 0
==================================================================================================
XP Starts: 27/6 (10h:5)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 58)       1280118     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 12)       1308        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 87)        1920177     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 35)        3815        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_196 (Concatenate)   (None, 50, 70)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 348)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 140)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 73)           31536       concatenate_196[0][0]            
__________________________________________________________________________________________________
concatenate_197 (Concatenate)   (None, 488)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_198 (Concatenate)   (None, 561)          0           phraseRnn[0][0]                  
                                                                 concatenate_197[0][0]            
__________________________________________________________________________________________________
dense_131 (Dense)               (None, 111)          62382       concatenate_198[0][0]            
__________________________________________________________________________________________________
dense_132 (Dense)               (None, 4)            448         dense_131[0][0]                  
==================================================================================================
Total params: 3,299,784
Trainable params: 3,299,784
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 404s - loss: 0.0746 - acc: 0.9816 - val_loss: 0.0593 - val_acc: 0.9842
Epoch 2/40
 - 369s - loss: 0.0504 - acc: 0.9872 - val_loss: 0.0604 - val_acc: 0.9842
Epoch 3/40
 - 385s - loss: 0.0478 - acc: 0.9881 - val_loss: 0.0629 - val_acc: 0.9841
Epoch 4/40
 - 365s - loss: 0.0463 - acc: 0.9886 - val_loss: 0.0687 - val_acc: 0.9840
Epoch 5/40
 - 364s - loss: 0.0453 - acc: 0.9888 - val_loss: 0.0744 - val_acc: 0.9838
Epoch 00005: early stopping
	TRAINING TIME: 34.53 minutes 
==================================================================================================
	PARSING TIME: 5.48 minutes 
==================================================================================================
	Identification : 0.465
	P, R  : 0.409, 0.539

==================================================================================================
	XP Ends: 27/6 (10 h:46)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,198            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.041          ,50             ,7              ,120            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
19             ,66             ,True           ,False          ,272            ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 198, True, 0.041, 50, 7, 120, 19, 66, True, False, 272
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (10h:46)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 120)      1372560     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        1064        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 66)        754908      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 19)        2888        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_199 (Concatenate)   (None, 50, 127)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 264)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 76)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 272)          326400      concatenate_199[0][0]            
__________________________________________________________________________________________________
concatenate_200 (Concatenate)   (None, 340)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_201 (Concatenate)   (None, 612)          0           phraseRnn[0][0]                  
                                                                 concatenate_200[0][0]            
__________________________________________________________________________________________________
dense_133 (Dense)               (None, 198)          121374      concatenate_201[0][0]            
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 4)            796         dense_133[0][0]                  
==================================================================================================
Total params: 2,579,990
Trainable params: 2,579,990
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 438s - loss: 12.0780 - acc: 0.2503 - val_loss: 12.1017 - val_acc: 0.2492
Epoch 2/40
 - 438s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.1017 - val_acc: 0.2492
Epoch 3/40
 - 437s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.1017 - val_acc: 0.2492
Epoch 4/40
 - 436s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.1017 - val_acc: 0.2492
Epoch 5/40
 - 438s - loss: 12.0857 - acc: 0.2502 - val_loss: 12.1017 - val_acc: 0.2492
Epoch 00005: early stopping
	TRAINING TIME: 38.1 minutes 
==================================================================================================
	PARSING TIME: 7.75 minutes 
==================================================================================================
	Identification : 0
	P, R  : 0.0, 0.0

==================================================================================================
	XP Ends: 27/6 (11 h:32)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (11h:32)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 12103
	After : 9401

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 9401 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 182
	MWE not in vocabulary 1082
	Dashed keys in vocabulary 2380
	One occurrence keys in vocabulary 2380 / 9401
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 120)      1128120     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        1183        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 66)        620466      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 19)        3211        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_202 (Concatenate)   (None, 50, 127)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 264)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 76)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 272)          326400      concatenate_202[0][0]            
__________________________________________________________________________________________________
concatenate_203 (Concatenate)   (None, 340)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_204 (Concatenate)   (None, 612)          0           phraseRnn[0][0]                  
                                                                 concatenate_203[0][0]            
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 198)          121374      concatenate_204[0][0]            
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 4)            796         dense_135[0][0]                  
==================================================================================================
Total params: 2,201,550
Trainable params: 2,201,550
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({0: 12221, 2: 12134, 3: 11962, 1: 11957})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 297s - loss: 0.1077 - acc: 0.9737 - val_loss: 0.0636 - val_acc: 0.9829
Epoch 2/40
 - 297s - loss: 0.0587 - acc: 0.9848 - val_loss: 0.0642 - val_acc: 0.9825
Epoch 3/40
 - 296s - loss: 0.0543 - acc: 0.9862 - val_loss: 0.0653 - val_acc: 0.9832
Epoch 4/40
 - 297s - loss: 0.0518 - acc: 0.9870 - val_loss: 0.0697 - val_acc: 0.9828
Epoch 5/40
 - 296s - loss: 0.0500 - acc: 0.9873 - val_loss: 0.0753 - val_acc: 0.9826
Epoch 00005: early stopping
	TRAINING TIME: 25.82 minutes 
==================================================================================================
	PARSING TIME: 14.35 minutes 
==================================================================================================
	Identification : 0.381
	P, R  : 0.28, 0.597

==================================================================================================
	XP Ends: 27/6 (12 h:13)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (12h:13)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 29784
	After : 22071

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 22071 * POS : 109
__________________________________________________________________________________________________
	Important words not in vocabulary 221
	MWE not in vocabulary 1768
	Dashed keys in vocabulary 3975
	One occurrence keys in vocabulary 3975 / 22071
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 120)      2648520     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 7)        763         phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 66)        1456686     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 19)        2071        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_205 (Concatenate)   (None, 50, 127)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 264)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 76)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 272)          326400      concatenate_205[0][0]            
__________________________________________________________________________________________________
concatenate_206 (Concatenate)   (None, 340)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_207 (Concatenate)   (None, 612)          0           phraseRnn[0][0]                  
                                                                 concatenate_206[0][0]            
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 198)          121374      concatenate_207[0][0]            
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 4)            796         dense_137[0][0]                  
==================================================================================================
Total params: 4,556,610
Trainable params: 4,556,610
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({2: 24103, 0: 24101, 3: 23949, 1: 23947})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 564s - loss: 0.0967 - acc: 0.9787 - val_loss: 0.0613 - val_acc: 0.9835
Epoch 2/40
 - 564s - loss: 0.0520 - acc: 0.9867 - val_loss: 0.0624 - val_acc: 0.9834
Epoch 3/40
 - 564s - loss: 0.0488 - acc: 0.9878 - val_loss: 0.0647 - val_acc: 0.9832
Epoch 4/40
 - 564s - loss: 0.0472 - acc: 0.9883 - val_loss: 0.0695 - val_acc: 0.9830
Epoch 5/40
 - 563s - loss: 0.0458 - acc: 0.9886 - val_loss: 0.0786 - val_acc: 0.9829
Epoch 00005: early stopping
	TRAINING TIME: 49.92 minutes 
==================================================================================================
	PARSING TIME: 5.58 minutes 
==================================================================================================
	Identification : 0.486
	P, R  : 0.408, 0.6

==================================================================================================
	XP Ends: 27/6 (13 h:9)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,69             ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.032          ,50             ,48             ,54             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
31             ,158            ,True           ,False          ,55             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 69, True, 0.032, 50, 48, 54, 31, 158, True, False, 55
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (13h:9)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 54)       407430      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 48)       7296        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 158)       1192110     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 31)        4712        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_208 (Concatenate)   (None, 50, 102)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 632)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 124)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 55)           26070       concatenate_208[0][0]            
__________________________________________________________________________________________________
concatenate_209 (Concatenate)   (None, 756)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_210 (Concatenate)   (None, 811)          0           phraseRnn[0][0]                  
                                                                 concatenate_209[0][0]            
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 69)           56028       concatenate_210[0][0]            
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 4)            280         dense_139[0][0]                  
==================================================================================================
Total params: 1,693,926
Trainable params: 1,693,926
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 271s - loss: 0.0651 - acc: 0.9829 - val_loss: 0.1360 - val_acc: 0.9579
Epoch 2/40
 - 271s - loss: 0.0513 - acc: 0.9871 - val_loss: 0.0561 - val_acc: 0.9856
Epoch 3/40
 - 269s - loss: 0.0480 - acc: 0.9883 - val_loss: 0.0583 - val_acc: 0.9857
Epoch 4/40
 - 271s - loss: 0.0464 - acc: 0.9887 - val_loss: 0.0604 - val_acc: 0.9857
Epoch 5/40
 - 270s - loss: 0.0452 - acc: 0.9891 - val_loss: 0.0659 - val_acc: 0.9854
Epoch 00005: early stopping
	TRAINING TIME: 24.13 minutes 
==================================================================================================
	PARSING TIME: 8.07 minutes 
==================================================================================================
	Identification : 0.576
	P, R  : 0.721, 0.479

==================================================================================================
	XP Ends: 27/6 (13 h:42)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (13h:42)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 54)       391014      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 48)       8112        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 158)       1144078     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 31)        5239        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_211 (Concatenate)   (None, 50, 102)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 632)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 124)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 55)           26070       concatenate_211[0][0]            
__________________________________________________________________________________________________
concatenate_212 (Concatenate)   (None, 756)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_213 (Concatenate)   (None, 811)          0           phraseRnn[0][0]                  
                                                                 concatenate_212[0][0]            
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 69)           56028       concatenate_213[0][0]            
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 4)            280         dense_141[0][0]                  
==================================================================================================
Total params: 1,630,821
Trainable params: 1,630,821
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 182s - loss: 0.0730 - acc: 0.9800 - val_loss: 0.0618 - val_acc: 0.9836
Epoch 2/40
 - 183s - loss: 0.0547 - acc: 0.9859 - val_loss: 0.0606 - val_acc: 0.9843
Epoch 3/40
 - 183s - loss: 0.0505 - acc: 0.9872 - val_loss: 0.0642 - val_acc: 0.9840
Epoch 4/40
 - 183s - loss: 0.0483 - acc: 0.9880 - val_loss: 0.0714 - val_acc: 0.9838
Epoch 5/40
 - 184s - loss: 0.0471 - acc: 0.9883 - val_loss: 0.0761 - val_acc: 0.9839
Epoch 00005: early stopping
	TRAINING TIME: 16.28 minutes 
==================================================================================================
	PARSING TIME: 12.48 minutes 
==================================================================================================
	Identification : 0.472
	P, R  : 0.399, 0.577

==================================================================================================
	XP Ends: 27/6 (14 h:11)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (14h:11)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 54)       744822      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 48)       5232        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 158)       2179294     transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 31)        3379        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_214 (Concatenate)   (None, 50, 102)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 632)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 124)          0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 55)           26070       concatenate_214[0][0]            
__________________________________________________________________________________________________
concatenate_215 (Concatenate)   (None, 756)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_216 (Concatenate)   (None, 811)          0           phraseRnn[0][0]                  
                                                                 concatenate_215[0][0]            
__________________________________________________________________________________________________
dense_143 (Dense)               (None, 69)           56028       concatenate_216[0][0]            
__________________________________________________________________________________________________
dense_144 (Dense)               (None, 4)            280         dense_143[0][0]                  
==================================================================================================
Total params: 3,015,105
Trainable params: 3,015,105
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 366s - loss: 0.0654 - acc: 0.9821 - val_loss: 0.0687 - val_acc: 0.9798
Epoch 2/40
 - 364s - loss: 0.0525 - acc: 0.9863 - val_loss: 0.0587 - val_acc: 0.9839
Epoch 3/40
 - 365s - loss: 0.0495 - acc: 0.9875 - val_loss: 0.0625 - val_acc: 0.9836
Epoch 4/40
 - 364s - loss: 0.0478 - acc: 0.9881 - val_loss: 0.0665 - val_acc: 0.9834
Epoch 5/40
 - 365s - loss: 0.0464 - acc: 0.9885 - val_loss: 0.0705 - val_acc: 0.9835
Epoch 00005: early stopping
	TRAINING TIME: 33.4 minutes 
==================================================================================================
	PARSING TIME: 5.5 minutes 
==================================================================================================
	Identification : 0.482
	P, R  : 0.446, 0.524

==================================================================================================
	XP Ends: 27/6 (14 h:50)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,True           ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,190            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.036          ,50             ,17             ,188            ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
7              ,30             ,True           ,True           ,26             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, True, True, False, 1, 1, 190, True, 0.036, 50, 17, 188, 7, 30, True, True, 26
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (14h:50)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9312
	After : 7545

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7545 * POS : 152
__________________________________________________________________________________________________
	Dashed keys in vocabulary 1622
	One occurrence keys in vocabulary 1622 / 7545
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 188)      1418460     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 17)       2584        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 30)        226350      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 7)         1064        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_217 (Concatenate)   (None, 50, 205)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 150)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 35)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 26)           18096       concatenate_217[0][0]            
__________________________________________________________________________________________________
concatenate_218 (Concatenate)   (None, 185)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_219 (Concatenate)   (None, 211)          0           phraseRnn[0][0]                  
                                                                 concatenate_218[0][0]            
__________________________________________________________________________________________________
dense_145 (Dense)               (None, 190)          40280       concatenate_219[0][0]            
__________________________________________________________________________________________________
dense_146 (Dense)               (None, 4)            764         dense_145[0][0]                  
==================================================================================================
Total params: 1,707,598
Trainable params: 1,707,598
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 18032, 2: 17815, 1: 17792, 3: 17658})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 271s - loss: 0.0641 - acc: 0.9832 - val_loss: 0.0547 - val_acc: 0.9860
Epoch 2/40
 - 271s - loss: 0.0498 - acc: 0.9876 - val_loss: 0.0546 - val_acc: 0.9860
Epoch 3/40
 - 270s - loss: 0.0467 - acc: 0.9887 - val_loss: 0.0571 - val_acc: 0.9860
Epoch 4/40
 - 270s - loss: 0.0451 - acc: 0.9892 - val_loss: 0.0607 - val_acc: 0.9858
Epoch 5/40
 - 270s - loss: 0.0442 - acc: 0.9894 - val_loss: 0.0651 - val_acc: 0.9858
Epoch 00005: early stopping
	TRAINING TIME: 24.12 minutes 
==================================================================================================
	PARSING TIME: 8.03 minutes 
==================================================================================================
	Identification : 0.571
	P, R  : 0.668, 0.499

==================================================================================================
	XP Ends: 27/6 (15 h:23)
==================================================================================================
ERROR:root:ATTENTION: Oracle problems with 2 sentences!
# Seed: 0
==================================================================================================
XP Starts: 27/6 (15h:23)
==================================================================================================

==================================================================================================
	PT Train (2305)
==================================================================================================
	Important sentence: 2305
	Token occurrences: 60342
	MWE number: 1412
	MWE occurrences: 2542
	Continuous occurrences: 58.0 %
	Frequent MWE occurences: 21.0 %
	MWE length: 2.22
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1
	Embedded occurrences: 5

==================================================================================================
	 Test (3117)
==================================================================================================
	Important sentence: 494
	Token occurrences: 64078
	MWE number: 429
	MWE occurrences: 553
	Continuous occurrences: 59.0 %
	MWE length: 2.25
	Seen occurrences : 62% 
	Recognizable MWEs: 98.0 %
	Embedded occurrences: 4
	Interleaving occurrences: 8

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 9194
	After : 7241

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 7241 * POS : 169
__________________________________________________________________________________________________
	Important words not in vocabulary 1
	MWE not in vocabulary 3
	Dashed keys in vocabulary 1674
	One occurrence keys in vocabulary 1674 / 7241
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 188)      1361308     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 17)       2873        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 30)        217230      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 7)         1183        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_220 (Concatenate)   (None, 50, 205)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 150)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 35)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 26)           18096       concatenate_220[0][0]            
__________________________________________________________________________________________________
concatenate_221 (Concatenate)   (None, 185)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_222 (Concatenate)   (None, 211)          0           phraseRnn[0][0]                  
                                                                 concatenate_221[0][0]            
__________________________________________________________________________________________________
dense_147 (Dense)               (None, 190)          40280       concatenate_222[0][0]            
__________________________________________________________________________________________________
dense_148 (Dense)               (None, 4)            764         dense_147[0][0]                  
==================================================================================================
Total params: 1,641,734
Trainable params: 1,641,734
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 123224
	data size after sampling = 241368
	4 Labels in train : Counter({0: 60342, 1: 60342, 2: 60342, 3: 60342})
	4 Labels in valid : Counter({1: 12186, 3: 12150, 0: 11973, 2: 11965})
Train on 193094 samples, validate on 48274 samples
Epoch 1/40
 - 184s - loss: 0.0735 - acc: 0.9800 - val_loss: 0.0597 - val_acc: 0.9843
Epoch 2/40
 - 184s - loss: 0.0523 - acc: 0.9869 - val_loss: 0.0591 - val_acc: 0.9848
Epoch 3/40
 - 184s - loss: 0.0485 - acc: 0.9882 - val_loss: 0.0619 - val_acc: 0.9848
Epoch 4/40
 - 183s - loss: 0.0470 - acc: 0.9886 - val_loss: 0.0652 - val_acc: 0.9843
Epoch 5/40
 - 183s - loss: 0.0463 - acc: 0.9887 - val_loss: 0.0692 - val_acc: 0.9843
Epoch 00005: early stopping
	TRAINING TIME: 16.3 minutes 
==================================================================================================
	PARSING TIME: 12.35 minutes 
==================================================================================================
	Identification : 0.488
	P, R  : 0.405, 0.613

==================================================================================================
	XP Ends: 27/6 (15 h:52)
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (15h:52)
==================================================================================================

==================================================================================================
	TR Train (3585)
==================================================================================================
	Important sentence: 3585
	Token occurrences: 120124
	MWE number: 2979
	MWE occurrences: 4870
	Continuous occurrences: 49.0 %
	Frequent MWE occurences: 24.0 %
	MWE length: 2.06
	Recognizable MWEs: 100.0 %
	MWT occurrences: 2

==================================================================================================
	 Test (1320)
==================================================================================================
	Important sentence: 385
	Token occurrences: 27196
	MWE number: 418
	MWE occurrences: 510
	Continuous occurrences: 50.0 %
	MWE length: 2.07
	Seen occurrences : 43% 
	Recognizable MWEs: 100.0 %

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 17288
	After : 13793

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 13793 * POS : 109
__________________________________________________________________________________________________
	Dashed keys in vocabulary 3161
	One occurrence keys in vocabulary 3161 / 13793
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 5)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 5)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 188)      2593084     phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 17)       1853        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 5, 30)        413790      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 5, 7)         763         transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_223 (Concatenate)   (None, 50, 205)      0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 150)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 35)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 26)           18096       concatenate_223[0][0]            
__________________________________________________________________________________________________
concatenate_224 (Concatenate)   (None, 185)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_225 (Concatenate)   (None, 211)          0           phraseRnn[0][0]                  
                                                                 concatenate_224[0][0]            
__________________________________________________________________________________________________
dense_149 (Dense)               (None, 190)          40280       concatenate_225[0][0]            
__________________________________________________________________________________________________
dense_150 (Dense)               (None, 4)            764         dense_149[0][0]                  
==================================================================================================
Total params: 3,068,630
Trainable params: 3,068,630
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 245118
	data size after sampling = 480496
	4 Labels in train : Counter({0: 120124, 1: 120124, 2: 120124, 3: 120124})
	4 Labels in valid : Counter({0: 24076, 2: 24044, 1: 24027, 3: 23953})
Train on 384396 samples, validate on 96100 samples
Epoch 1/40
 - 366s - loss: 0.0656 - acc: 0.9822 - val_loss: 0.0569 - val_acc: 0.9846
Epoch 2/40
 - 366s - loss: 0.0515 - acc: 0.9868 - val_loss: 0.0574 - val_acc: 0.9845
Epoch 3/40
 - 364s - loss: 0.0484 - acc: 0.9880 - val_loss: 0.0606 - val_acc: 0.9844
Epoch 4/40
 - 364s - loss: 0.0466 - acc: 0.9886 - val_loss: 0.0649 - val_acc: 0.9840
Epoch 5/40
 - 364s - loss: 0.0455 - acc: 0.9889 - val_loss: 0.0687 - val_acc: 0.9840
Epoch 00005: early stopping
	TRAINING TIME: 33.3 minutes 
==================================================================================================
	PARSING TIME: 5.52 minutes 
==================================================================================================
	Identification : 0.497
	P, R  : 0.464, 0.535

==================================================================================================
	XP Ends: 27/6 (16 h:32)
==================================================================================================
	Mode: MLPPHRASE
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, favorisationCoeff, focused, importantSentences, importantTransitions, mweRepeition, overSampling, sampleWeight, average, compactVocab, dynamicVocab, keras, lemma, manual, pretrained, useB-1, useB1, denseUnitNumber, gru, lr, phraseMaxLength, phrasePosEmb, phraseTokenEmb, transPosEmb, transTokenEmb, useB-1, useB1, wordRnnUnitNum
==================================================================================================
xp             ,Dataset        ,Evaluation     ,favorisationCoe,focused        ,
__________________________________________________________________________________________________
mlpPhrase      ,sharedtask2    ,fixedSize      ,1              ,False          ,
__________________________________________________________________________________________________
importantSenten,importantTransi,mweRepeition   ,overSampling   ,sampleWeight   ,
__________________________________________________________________________________________________
True           ,False          ,35             ,True           ,False          ,
__________________________________________________________________________________________________
average        ,compactVocab   ,dynamicVocab   ,keras          ,lemma          ,
__________________________________________________________________________________________________
True           ,False          ,False          ,False          ,False          ,
__________________________________________________________________________________________________
manual         ,pretrained     ,useB-1         ,useB1          ,denseUnitNumber,
__________________________________________________________________________________________________
True           ,False          ,1              ,1              ,113            ,
__________________________________________________________________________________________________
gru            ,lr             ,phraseMaxLength,phrasePosEmb   ,phraseTokenEmb ,
__________________________________________________________________________________________________
True           ,0.012          ,50             ,11             ,28             ,
__________________________________________________________________________________________________
transPosEmb    ,transTokenEmb  ,useB-1         ,useB1          ,wordRnnUnitNum ,
__________________________________________________________________________________________________
15             ,86             ,False          ,True           ,45             ,
__________________________________________________________________________________________________

__________________________________________________________________________________________________

__________________________________________________________________________________________________
# Configs: mlpPhrase, sharedtask2, fixedSize, 1, False, True, False, 35, True, False, True, False, False, False, False, True, False, 1, 1, 113, True, 0.012, 50, 11, 28, 15, 86, False, True, 45
==================================================================================================
# Seed: 0
==================================================================================================
XP Starts: 27/6 (16h:32)
==================================================================================================

==================================================================================================
	BG Train (3124)
==================================================================================================
	Important sentence: 3124
	Token occurrences: 89121
	MWE number: 1372
	MWE occurrences: 3583
	Continuous occurrences: 81.0 %
	Frequent MWE occurences: 49.0 %
	MWE length: 2.12
	Recognizable MWEs: 100.0 %
	MWT occurrences: 8

==================================================================================================
	 Test (1954)
==================================================================================================
	Important sentence: 574
	Token occurrences: 42020
	MWE number: 451
	MWE occurrences: 670
	Continuous occurrences: 75.0 %
	MWE length: 2.14
	Seen occurrences : 57% 
	Recognizable MWEs: 100.0 %
	MWT occurrences: 1

==================================================================================================
	Non frequent word cleaning:
==================================================================================================
	Before : 14798
	After : 11438

__________________________________________________________________________________________________
	Vocabulary
==================================================================================================
	Tokens := 11438 * POS : 152
__________________________________________________________________________________________________
	Important words not in vocabulary 675
	MWE not in vocabulary 1297
	Dashed keys in vocabulary 2182
	One occurrence keys in vocabulary 2182 / 11438
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
phraseTokens (InputLayer)       (None, 50)           0                                            
__________________________________________________________________________________________________
phrasePoss (InputLayer)         (None, 50)           0                                            
__________________________________________________________________________________________________
transTokens (InputLayer)        (None, 4)            0                                            
__________________________________________________________________________________________________
transPoss (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
phraseTokenEmb (Embedding)      (None, 50, 28)       320264      phraseTokens[0][0]               
__________________________________________________________________________________________________
phrasePosEmb (Embedding)        (None, 50, 11)       1672        phrasePoss[0][0]                 
__________________________________________________________________________________________________
transTokenEmb (Embedding)       (None, 4, 86)        983668      transTokens[0][0]                
__________________________________________________________________________________________________
transPosEmb (Embedding)         (None, 4, 15)        2280        transPoss[0][0]                  
__________________________________________________________________________________________________
concatenate_226 (Concatenate)   (None, 50, 39)       0           phraseTokenEmb[0][0]             
                                                                 phrasePosEmb[0][0]               
__________________________________________________________________________________________________
transTokenFlat (Flatten)        (None, 344)          0           transTokenEmb[0][0]              
__________________________________________________________________________________________________
transPosFlat (Flatten)          (None, 60)           0           transPosEmb[0][0]                
__________________________________________________________________________________________________
phraseRnn (GRU)                 (None, 45)           11475       concatenate_226[0][0]            
__________________________________________________________________________________________________
concatenate_227 (Concatenate)   (None, 404)          0           transTokenFlat[0][0]             
                                                                 transPosFlat[0][0]               
__________________________________________________________________________________________________
concatenate_228 (Concatenate)   (None, 449)          0           phraseRnn[0][0]                  
                                                                 concatenate_227[0][0]            
__________________________________________________________________________________________________
dense_151 (Dense)               (None, 113)          50850       concatenate_228[0][0]            
__________________________________________________________________________________________________
dense_152 (Dense)               (None, 4)            456         dense_151[0][0]                  
==================================================================================================
Total params: 1,370,665
Trainable params: 1,370,665
Non-trainable params: 0
__________________________________________________________________________________________________
None
__________________________________________________________________________________________________
	Sampling
==================================================================================================

==================================================================================================
	Resampling:
==================================================================================================
	data size before sampling = 181825
	data size after sampling = 356484
	4 Labels in train : Counter({0: 89121, 1: 89121, 2: 89121, 3: 89121})
	4 Labels in valid : Counter({0: 17893, 3: 17879, 1: 17774, 2: 17751})
Train on 285187 samples, validate on 71297 samples
Epoch 1/40
 - 269s - loss: 0.0721 - acc: 0.9811 - val_loss: 0.0575 - val_acc: 0.9853
Epoch 2/40
 - 268s - loss: 0.0520 - acc: 0.9872 - val_loss: 0.0567 - val_acc: 0.9857
Epoch 3/40
 - 268s - loss: 0.0484 - acc: 0.9883 - val_loss: 0.0574 - val_acc: 0.9859
Epoch 4/40
 - 270s - loss: 0.0468 - acc: 0.9887 - val_loss: 0.0938 - val_acc: 0.9797
## OAR [2019-06-27 16:55:42] Job 1982520 KILLED ##
