Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:
/tmp/try_flags_YOANpO.c:4:10: fatal error: cudnn.h: No such file or directory
 #include <cudnn.h>
          ^~~~~~~~~
compilation terminated.

Preallocating 10869/11441 Mb (0.950000) on cuda
Mapped name None to device cuda: Tesla K40m (0000:03:00.0)
/home/halsaied/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
	TransitionClassifier(
  (p_embeddings): Embedding(18, 30)
  (w_embeddings): Embedding(1177, 89)
  (rnn): GRU(119, 60, bidirectional=True)
  (linear1): Linear(in_features=960, out_features=30, bias=True)
  (linear2): Linear(in_features=30, out_features=4, bias=True)
)
==================================================================================================
	Adagrad (
Parameter Group 0
    initial_accumulator_value: 0
    lr: 0.07
    lr_decay: 0
    weight_decay: 0
)
==================================================================================================
	Mode: KIPERWASSER
==================================================================================================
	Dataset: SHAREDTASK2
==================================================================================================
	Division: FIXEDSIZE
==================================================================================================
	GPU Enabled
==================================================================================================
# CTitles: xp, Dataset, Evaluation, lemma, compactVocab, sampleWeight, importantSentences, importantTransitions, favorisationCoeff, focused, overSampling, rnnLayerNum, verbose, focusedElemNum, sampling, epochs, dense1, file, dense2, earlyStop, moreTrans, gru, eager, lr, rnnDropout, posDim, optimizer, denseDropout, layerNum, dropout, earlyStopPatience, denseActivation, wordDim, batch, trainValidationSet, rnnUnitNum
==================================================================================================
# Configs: kiperwasser, sharedtask2, fixedSize, True, True, False, True, False, 1, False, False, 1, True, 8, True, 5, 30, kiper.p, 0, False, True, True, True, 0.07, 0.2, 30, adagrad, False, 2, 0.3, 7, tanh, 89, 1, True, 60
==================================================================================================
	Language : BG
==================================================================================================
	Training (Important) : 3124, Test : 1954
	MWEs in tain : 1372, occurrences : 3583
	Impotant words in tain : 1175
	MWE length mean : 2.13
	Seen MWEs : 386 (57 %)
	New MWEs : 284 (42 %)
==================================================================================================
# Network optimizer = Adagrad, learning rate = 0.07

	Best model path: Reports/423.kiper.p
__________________________________________________________________________________________________
Epoch 0:  Total loss = 25911.857133 on 2811
Validation loss: 2310.734648, Identification accuracy: 0.837000
Epoch 1:  Total loss = 17404.317113 on 2811
Validation loss: 933.140791, Identification accuracy: 0.287000
Epoch 2:  Total loss = 13170.460472 on 2811
Validation loss: 569.262722, Identification accuracy: 0.000000
Epoch 3:  Total loss = 11252.350537 on 2811
Validation loss: 301.009510, Identification accuracy: 0.000000
Epoch 4:  Total loss = 9045.548240 on 2811
Validation loss: 242.680819, Identification accuracy: 0.000000
	Training validation set!
__________________________________________________________________________________________________
# Network optimizer = Adagrad, learning rate = 0.07

==================================================================================================
	Training time : 5:22:08.921258
==================================================================================================

==================================================================================================
	Parsing time : 0:01:01.871803
==================================================================================================
	Identification : 0

*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|*|
